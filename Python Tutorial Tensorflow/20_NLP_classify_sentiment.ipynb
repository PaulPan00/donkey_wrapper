{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'love': 2,\n",
       " 'my': 3,\n",
       " 'i': 4,\n",
       " 'dog': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'really': 8,\n",
       " 'goof': 9,\n",
       " 'me': 10,\n",
       " 'off': 11}"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenize our sentence\n",
    "sentences = [\n",
    "    \"I love my dog\",\n",
    "    \"I love my cat\",\n",
    "    \"You love my dog!\",\n",
    "    \"YOu really goof me off?!\"\n",
    "]\n",
    "\n",
    "test_data = [\"i really love my dog huh?\",\n",
    "    \"my dog loves my manatee\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[4, 2, 3, 5], [4, 2, 3, 7], [6, 2, 3, 5], [6, 8, 9, 10, 11]]"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "# Manage sequencing, creating sequences of words in each sentence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[4, 8, 2, 3, 5, 1], [3, 5, 1, 3, 1]]"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "# missing data in training set, use `oov_token=\"<OOV>\"`\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 4,  2,  3,  5,  0,  0],\n",
       "       [ 4,  2,  3,  7,  0,  0],\n",
       "       [ 6,  2,  3,  5,  0,  0],\n",
       "       [ 6,  8,  9, 10, 11,  0]])"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "# Add padding, make every sentence same length by padding zeros\n",
    "padded = pad_sequences(sequences, padding=\"post\", truncating=\"post\", maxlen=6)\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['thirtysomething scientists unveil doomsday clock of hair loss',\n",
       " 'dem rep. totally nails why congress is falling short on gender, racial equality',\n",
       " 'eat your veggies: 9 deliciously different recipes',\n",
       " 'inclement weather prevents liar from getting to work',\n",
       " \"mother comes pretty close to using word 'streaming' correctly\"]"
      ]
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# get our dataset\n",
    "import json\n",
    "\n",
    "datastore = []\n",
    "with open(\"20_Sarcasm_Headlines_Dataset_v2.txt\", \"r\") as f:\n",
    "    for x in f.readlines():\n",
    "        datastore.append(eval(x))\n",
    "\n",
    "sentences, labels, urls = [], [], []\n",
    "\n",
    "for item in datastore:\n",
    "    sentences.append(item[\"headline\"])\n",
    "    labels.append(item[\"is_sarcastic\"])\n",
    "    urls.append(item[\"article_link\"])\n",
    "\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([[1, 325, 3169, 5817, 2489, 3, 655, 993],\n",
       "  [5818, 1723, 735, 2490, 47, 248, 11, 1824, 919, 8, 1825, 2032, 2297]],\n",
       " array([   1,  325, 3169, 5817, 2489,    3,  655,  993,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]))"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "training_sequences[:2], training_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_padded = np.array(training_padded)\n",
    "training_labels = np.array(training_labels)\n",
    "testing_padded = np.array(testing_padded)\n",
    "testing_labels = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding and model\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(24, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 100, 16)           160000    \n_________________________________________________________________\nglobal_average_pooling1d_1 ( (None, 16)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 24)                408       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 25        \n=================================================================\nTotal params: 160,433\nTrainable params: 160,433\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1518 - accuracy: 0.9459 - val_loss: 0.3743 - val_accuracy: 0.8445\n",
      "Epoch 2/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1373 - accuracy: 0.9510 - val_loss: 0.3820 - val_accuracy: 0.8460\n",
      "Epoch 3/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1271 - accuracy: 0.9556 - val_loss: 0.4020 - val_accuracy: 0.8402\n",
      "Epoch 4/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1161 - accuracy: 0.9607 - val_loss: 0.4228 - val_accuracy: 0.8372\n",
      "Epoch 5/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.1075 - accuracy: 0.9635 - val_loss: 0.4359 - val_accuracy: 0.8390\n",
      "Epoch 6/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0999 - accuracy: 0.9671 - val_loss: 0.4809 - val_accuracy: 0.8284\n",
      "Epoch 7/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0914 - accuracy: 0.9707 - val_loss: 0.4807 - val_accuracy: 0.8334\n",
      "Epoch 8/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0863 - accuracy: 0.9709 - val_loss: 0.5026 - val_accuracy: 0.8311\n",
      "Epoch 9/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0789 - accuracy: 0.9747 - val_loss: 0.5548 - val_accuracy: 0.8269\n",
      "Epoch 10/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0719 - accuracy: 0.9783 - val_loss: 0.5543 - val_accuracy: 0.8270\n",
      "Epoch 11/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0676 - accuracy: 0.9790 - val_loss: 0.5839 - val_accuracy: 0.8234\n",
      "Epoch 12/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0630 - accuracy: 0.9807 - val_loss: 0.6133 - val_accuracy: 0.8233\n",
      "Epoch 13/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0572 - accuracy: 0.9834 - val_loss: 0.6403 - val_accuracy: 0.8189\n",
      "Epoch 14/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0531 - accuracy: 0.9851 - val_loss: 0.6707 - val_accuracy: 0.8196\n",
      "Epoch 15/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0500 - accuracy: 0.9858 - val_loss: 0.7076 - val_accuracy: 0.8190\n",
      "Epoch 16/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0467 - accuracy: 0.9871 - val_loss: 0.7402 - val_accuracy: 0.8140\n",
      "Epoch 17/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0443 - accuracy: 0.9877 - val_loss: 0.7595 - val_accuracy: 0.8165\n",
      "Epoch 18/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9887 - val_loss: 0.7920 - val_accuracy: 0.8160\n",
      "Epoch 19/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9900 - val_loss: 0.8281 - val_accuracy: 0.8125\n",
      "Epoch 20/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9917 - val_loss: 0.8688 - val_accuracy: 0.8095\n",
      "Epoch 21/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9920 - val_loss: 0.8843 - val_accuracy: 0.8126\n",
      "Epoch 22/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9924 - val_loss: 0.9144 - val_accuracy: 0.8115\n",
      "Epoch 23/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0276 - accuracy: 0.9935 - val_loss: 0.9463 - val_accuracy: 0.8101\n",
      "Epoch 24/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9937 - val_loss: 0.9781 - val_accuracy: 0.8089\n",
      "Epoch 25/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0247 - accuracy: 0.9944 - val_loss: 1.0258 - val_accuracy: 0.8075\n",
      "Epoch 26/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9948 - val_loss: 1.0873 - val_accuracy: 0.8043\n",
      "Epoch 27/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9948 - val_loss: 1.1626 - val_accuracy: 0.8021\n",
      "Epoch 28/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9949 - val_loss: 1.1284 - val_accuracy: 0.8035\n",
      "Epoch 29/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9957 - val_loss: 1.2123 - val_accuracy: 0.8016\n",
      "Epoch 30/30\n",
      "625/625 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 1.1697 - val_accuracy: 0.8006\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.001]\n [0.   ]\n [0.   ]\n [0.006]\n [0.002]\n [0.2  ]\n [1.   ]]\n"
     ]
    }
   ],
   "source": [
    "sacarstic_sentence = [\n",
    "    \"I dear you not you lil goof, shame on you\",\n",
    "    \"Ok I like it\",\n",
    "    \"Do you hate me? Yes\",\n",
    "    \"I Loved it!!!\",\n",
    "    \"Jason\",\n",
    "    \"Paul is good\",\n",
    "    \"Fuck\"\n",
    "]\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sacarstic_sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "print(np.round(model.predict(padded), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}