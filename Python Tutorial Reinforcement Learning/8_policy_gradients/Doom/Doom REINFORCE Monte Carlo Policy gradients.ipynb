{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Doom-Health: REINFORCE Monte Carlo Policy gradients "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook we'll implement an agent <b>that try to survive in Doom environment by using a Policy Gradient architecture.</b> <br>\r\n",
    "Our agent playing Doom:\r\n",
    "\r\n",
    "<img src=\"assets/projectw4.gif\" style=\"max-width: 600px;\" alt=\"Policy Gradient with Doom\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# You can follow this notebook with this video tutorial  that will helps you to understand each step:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import the libraries ðŸ“š"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf      # Deep Learning library\r\n",
    "import numpy as np           # Handle matrices\r\n",
    "from vizdoom import *        # Doom Environment\r\n",
    "import random                # Handling random number generation\r\n",
    "import time                  # Handling time calculation\r\n",
    "from skimage import transform# Help us to preprocess the frames\r\n",
    "\r\n",
    "from collections import deque# Ordered collection with ends\r\n",
    "import matplotlib.pyplot as plt # Display graphs\r\n",
    "\r\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create our environment ðŸŽ®\r\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\r\n",
    "- Doom environment takes:\r\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\r\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\r\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \r\n",
    "\r\n",
    "### Our environment\r\n",
    "<img src=\"assets/health_doom.jpg\" style=\"max-width:500px;\" alt=\"Doom health\"/>\r\n",
    "\r\n",
    "The purpose of this scenario is to teach the agent **how to survive without knowing what makes him survive.** Agent know only that life is precious and death is bad so he must learn what prolongs his existence and that his health is connected with it.\r\n",
    "\r\n",
    "Map is a rectangle with green, acidic floor which hurts the player periodically. Initially there are some medkits spread uniformly over the map. A new medkit falls from the skies every now and then. **Medkits heal some portions of player's health - to survive agent needs to pick them up. Episode finishes after player's death or on timeout.**\r\n",
    "\r\n",
    "Further configuration:\r\n",
    "\r\n",
    "- living_reward = 1\r\n",
    "- 3 available buttons: turn left, turn right, move forward\r\n",
    "- 1 available game variable: HEALTH\r\n",
    "- death penalty = 100\r\n",
    "<br><br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\"\r\n",
    "Here we create our environment\r\n",
    "\"\"\"\r\n",
    "def create_environment():\r\n",
    "    game = DoomGame()\r\n",
    "    \r\n",
    "    # Load the correct configuration\r\n",
    "    game.load_config(\"health_gathering.cfg\")\r\n",
    "    \r\n",
    "    # Load the correct scenario (in our case defend_the_center scenario)\r\n",
    "    game.set_doom_scenario_path(\"health_gathering.wad\")\r\n",
    "    \r\n",
    "    game.init()\r\n",
    "    \r\n",
    "    # Here our possible actions\r\n",
    "    # [[1,0,0],[0,1,0],[0,0,1]]\r\n",
    "    possible_actions  = np.identity(3,dtype=int).tolist()\r\n",
    "    \r\n",
    "    return game, possible_actions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "game, possible_actions = create_environment()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define the preprocessing functions \r\n",
    "### preprocess_frame \r\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\r\n",
    "<br><br>\r\n",
    "Our steps:\r\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\r\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\r\n",
    "- We normalize pixel values\r\n",
    "- Finally we resize the preprocessed frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\"\r\n",
    "    preprocess_frame:\r\n",
    "    Take a frame.\r\n",
    "    Resize it.\r\n",
    "        __________________\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |_________________|\r\n",
    "        \r\n",
    "        to\r\n",
    "        _____________\r\n",
    "        |            |\r\n",
    "        |            |\r\n",
    "        |            |\r\n",
    "        |____________|\r\n",
    "    Normalize it.\r\n",
    "    \r\n",
    "    return preprocessed_frame\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "def preprocess_frame(frame):\r\n",
    "    # Greyscale frame already done in our vizdoom config\r\n",
    "    # x = np.mean(frame,-1)\r\n",
    "    \r\n",
    "    # Crop the screen (remove the roof because it contains no information)\r\n",
    "    # [Up: Down, Left: right]\r\n",
    "    cropped_frame = frame[80:,:]\r\n",
    "    \r\n",
    "    # Normalize Pixel Values\r\n",
    "    normalized_frame = cropped_frame/255.0\r\n",
    "    \r\n",
    "    # Resize\r\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84,84])\r\n",
    "    \r\n",
    "    return preprocessed_frame"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### stack_frames\r\n",
    " This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\r\n",
    "\r\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\r\n",
    "\r\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\r\n",
    "\r\n",
    "- First we preprocess frame\r\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\r\n",
    "- Finally we **build the stacked state**\r\n",
    "\r\n",
    "This is how work stack:\r\n",
    "- For the first frame, we feed 4 frames\r\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\r\n",
    "- And so on\r\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\r\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "stack_size = 4 # We stack 4 frames\r\n",
    "\r\n",
    "# Initialize deque with zero-images one array for each image\r\n",
    "stacked_frames  =  deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4) \r\n",
    "\r\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\r\n",
    "    # Preprocess frame\r\n",
    "    frame = preprocess_frame(state)\r\n",
    "    \r\n",
    "    if is_new_episode:\r\n",
    "        # Clear our stacked_frames\r\n",
    "        stacked_frames = deque([np.zeros((84,84), dtype=np.int) for i in range(stack_size)], maxlen=4)\r\n",
    "        \r\n",
    "        # Because we're in a new episode, copy the same frame 4x\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        \r\n",
    "        # Stack the frames\r\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\r\n",
    "\r\n",
    "    else:\r\n",
    "        # Append frame to deque, automatically removes the oldest frame\r\n",
    "        stacked_frames.append(frame)\r\n",
    "\r\n",
    "        # Build the stacked state (first dimension specifies different frames)\r\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \r\n",
    "    \r\n",
    "    return stacked_state, stacked_frames"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### discount_and_normalize_rewards ðŸ’°\r\n",
    "This function is important, because we are in a Monte Carlo situation. <br>\r\n",
    "\r\n",
    "We need to **discount the rewards at the end of the episode**. This function takes, the reward discount it, and **then normalize them** (to avoid a big variability in rewards)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\r\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\r\n",
    "    cumulative = 0.0\r\n",
    "    for i in reversed(range(len(episode_rewards))):\r\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\r\n",
    "        discounted_episode_rewards[i] = cumulative\r\n",
    "    \r\n",
    "    mean = np.mean(discounted_episode_rewards)\r\n",
    "    std = np.std(discounted_episode_rewards)\r\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\r\n",
    "\r\n",
    "    return discounted_episode_rewards"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Set up our hyperparameters \r\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\r\n",
    "\r\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\r\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### ENVIRONMENT HYPERPARAMETERS\r\n",
    "state_size = [84,84,4] # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \r\n",
    "action_size = game.get_available_buttons_size() # 3 possible actions: turn left, turn right, move forward\r\n",
    "stack_size = 4 # Defines how many frames are stacked together\r\n",
    "\r\n",
    "## TRAINING HYPERPARAMETERS\r\n",
    "learning_rate = 0.002\r\n",
    "num_epochs = 500 # Total epochs for training \r\n",
    "\r\n",
    "batch_size = 1000 # Each 1 is a timestep (NOT AN EPISODE) # YOU CAN CHANGE TO 5000 if you have GPU\r\n",
    "gamma = 0.95 # Discounting rate\r\n",
    "\r\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\r\n",
    "training = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Quick note: Policy gradient methods like reinforce **are on-policy method which can not be updated from experience replay.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model ðŸ§ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"assets/doomPG.png\" alt=\"Doom PG\"/>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class PGNetwork:\r\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='PGNetwork'):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size = action_size\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        \r\n",
    "        with tf.variable_scope(name):\r\n",
    "            with tf.name_scope(\"inputs\"):\r\n",
    "                # We create the placeholders\r\n",
    "                # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\r\n",
    "                # [None, 84, 84, 4]\r\n",
    "                self.inputs_= tf.placeholder(tf.float32, [None, *state_size], name=\"inputs_\")\r\n",
    "                self.actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\r\n",
    "                self.discounted_episode_rewards_ = tf.placeholder(tf.float32, [None, ], name=\"discounted_episode_rewards_\")\r\n",
    "            \r\n",
    "                \r\n",
    "                # Add this placeholder for having this variable in tensorboard\r\n",
    "                self.mean_reward_ = tf.placeholder(tf.float32, name=\"mean_reward\")\r\n",
    "                \r\n",
    "            with tf.name_scope(\"conv1\"):\r\n",
    "                \"\"\"\r\n",
    "                First convnet:\r\n",
    "                CNN\r\n",
    "                BatchNormalization\r\n",
    "                ELU\r\n",
    "                \"\"\"\r\n",
    "                # Input is 84x84x4\r\n",
    "                self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\r\n",
    "                                             filters = 32,\r\n",
    "                                             kernel_size = [8,8],\r\n",
    "                                             strides = [4,4],\r\n",
    "                                             padding = \"VALID\",\r\n",
    "                                              kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                             name = \"conv1\")\r\n",
    "\r\n",
    "                self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\r\n",
    "                                                       training = True,\r\n",
    "                                                       epsilon = 1e-5,\r\n",
    "                                                         name = 'batch_norm1')\r\n",
    "\r\n",
    "                self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\r\n",
    "                ## --> [20, 20, 32]\r\n",
    "            \r\n",
    "            with tf.name_scope(\"conv2\"):\r\n",
    "                \"\"\"\r\n",
    "                Second convnet:\r\n",
    "                CNN\r\n",
    "                BatchNormalization\r\n",
    "                ELU\r\n",
    "                \"\"\"\r\n",
    "                self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\r\n",
    "                                     filters = 64,\r\n",
    "                                     kernel_size = [4,4],\r\n",
    "                                     strides = [2,2],\r\n",
    "                                     padding = \"VALID\",\r\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                     name = \"conv2\")\r\n",
    "\r\n",
    "                self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\r\n",
    "                                                       training = True,\r\n",
    "                                                       epsilon = 1e-5,\r\n",
    "                                                         name = 'batch_norm2')\r\n",
    "\r\n",
    "                self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\r\n",
    "                ## --> [9, 9, 64]\r\n",
    "            \r\n",
    "            with tf.name_scope(\"conv3\"):\r\n",
    "                \"\"\"\r\n",
    "                Third convnet:\r\n",
    "                CNN\r\n",
    "                BatchNormalization\r\n",
    "                ELU\r\n",
    "                \"\"\"\r\n",
    "                self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\r\n",
    "                                     filters = 128,\r\n",
    "                                     kernel_size = [4,4],\r\n",
    "                                     strides = [2,2],\r\n",
    "                                     padding = \"VALID\",\r\n",
    "                                    kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                     name = \"conv3\")\r\n",
    "\r\n",
    "                self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\r\n",
    "                                                       training = True,\r\n",
    "                                                       epsilon = 1e-5,\r\n",
    "                                                         name = 'batch_norm3')\r\n",
    "\r\n",
    "                self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\r\n",
    "                ## --> [3, 3, 128]\r\n",
    "            \r\n",
    "            with tf.name_scope(\"flatten\"):\r\n",
    "                self.flatten = tf.layers.flatten(self.conv3_out)\r\n",
    "                ## --> [1152]\r\n",
    "            \r\n",
    "            with tf.name_scope(\"fc1\"):\r\n",
    "                self.fc = tf.layers.dense(inputs = self.flatten,\r\n",
    "                                      units = 512,\r\n",
    "                                      activation = tf.nn.elu,\r\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
    "                                    name=\"fc1\")\r\n",
    "            \r\n",
    "            with tf.name_scope(\"logits\"):\r\n",
    "                self.logits = tf.layers.dense(inputs = self.fc, \r\n",
    "                                               kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
    "                                              units = 3, \r\n",
    "                                            activation=None)\r\n",
    "            \r\n",
    "            with tf.name_scope(\"softmax\"):\r\n",
    "                self.action_distribution = tf.nn.softmax(self.logits)\r\n",
    "                \r\n",
    "\r\n",
    "            with tf.name_scope(\"loss\"):\r\n",
    "                # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function\r\n",
    "                # If you have single-class labels, where an object can only belong to one class, you might now consider using \r\n",
    "                # tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array. \r\n",
    "                self.neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = self.logits, labels = self.actions)\r\n",
    "                self.loss = tf.reduce_mean(self.neg_log_prob * self.discounted_episode_rewards_) \r\n",
    "        \r\n",
    "    \r\n",
    "            with tf.name_scope(\"train\"):\r\n",
    "                self.train_opt = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Reset the graph\r\n",
    "tf.reset_default_graph()\r\n",
    "\r\n",
    "# Instantiate the PGNetwork\r\n",
    "PGNetwork = PGNetwork(state_size, action_size, learning_rate)\r\n",
    "\r\n",
    "# Initialize Session\r\n",
    "sess = tf.Session()\r\n",
    "init = tf.global_variables_initializer()\r\n",
    "sess.run(init)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Set up Tensorboard \r\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\r\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Setup TensorBoard Writer\r\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/pg/test\")\r\n",
    "\r\n",
    "## Losses\r\n",
    "tf.summary.scalar(\"Loss\", PGNetwork.loss)\r\n",
    "\r\n",
    "## Reward mean\r\n",
    "tf.summary.scalar(\"Reward_mean\", PGNetwork.mean_reward_ )\r\n",
    "\r\n",
    "write_op = tf.summary.merge_all()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Train our Agent "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we'll create batches.<br>\r\n",
    "These batches contains episodes **(their number depends on how many rewards we collect**: for instance if we have episodes with only 10 rewards we can put batch_size/10 episodes\r\n",
    "<br>\r\n",
    "* Make a batch\r\n",
    "    * For each step:\r\n",
    "        * Choose action a\r\n",
    "        * Perform action a\r\n",
    "        * Store s, a, r\r\n",
    "        * **If** done:\r\n",
    "            * Calculate sum reward\r\n",
    "            * Calculate gamma Gt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def make_batch(batch_size, stacked_frames):\r\n",
    "    # Initialize lists: states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards\r\n",
    "    states, actions, rewards_of_episode, rewards_of_batch, discounted_rewards = [], [], [], [], []\r\n",
    "    \r\n",
    "    # Reward of batch is also a trick to keep track of how many timestep we made.\r\n",
    "    # We use to to verify at the end of each episode if > batch_size or not.\r\n",
    "    \r\n",
    "    # Keep track of how many episodes in our batch (useful when we'll need to calculate the average reward per episode)\r\n",
    "    episode_num  = 1\r\n",
    "    \r\n",
    "    # Launch a new episode\r\n",
    "    game.new_episode()\r\n",
    "        \r\n",
    "    # Get a new state\r\n",
    "    state = game.get_state().screen_buffer\r\n",
    "    state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "\r\n",
    "    while True:\r\n",
    "        # Run State Through Policy & Calculate Action\r\n",
    "        action_probability_distribution = sess.run(PGNetwork.action_distribution, \r\n",
    "                                                   feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\r\n",
    "        \r\n",
    "        # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\r\n",
    "        # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\r\n",
    "        #30% chance that we take action a2)\r\n",
    "        action = np.random.choice(range(action_probability_distribution.shape[1]), \r\n",
    "                                  p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\r\n",
    "        action = possible_actions[action]\r\n",
    "\r\n",
    "        # Perform action\r\n",
    "        reward = game.make_action(action)\r\n",
    "        done = game.is_episode_finished()\r\n",
    "\r\n",
    "        # Store results\r\n",
    "        states.append(state)\r\n",
    "        actions.append(action)\r\n",
    "        rewards_of_episode.append(reward)\r\n",
    "        \r\n",
    "        if done:\r\n",
    "            # The episode ends so no next state\r\n",
    "            next_state = np.zeros((84, 84), dtype=np.int)\r\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "            \r\n",
    "            # Append the rewards_of_batch to reward_of_episode\r\n",
    "            rewards_of_batch.append(rewards_of_episode)\r\n",
    "            \r\n",
    "            # Calculate gamma Gt\r\n",
    "            discounted_rewards.append(discount_and_normalize_rewards(rewards_of_episode))\r\n",
    "           \r\n",
    "            # If the number of rewards_of_batch > batch_size stop the minibatch creation\r\n",
    "            # (Because we have sufficient number of episode mb)\r\n",
    "            # Remember that we put this condition here, because we want entire episode (Monte Carlo)\r\n",
    "            # so we can't check that condition for each step but only if an episode is finished\r\n",
    "            if len(np.concatenate(rewards_of_batch)) > batch_size:\r\n",
    "                break\r\n",
    "                \r\n",
    "            # Reset the transition stores\r\n",
    "            rewards_of_episode = []\r\n",
    "            \r\n",
    "            # Add episode\r\n",
    "            episode_num += 1\r\n",
    "            \r\n",
    "            # Start a new episode\r\n",
    "            game.new_episode()\r\n",
    "\r\n",
    "            # First we need a state\r\n",
    "            state = game.get_state().screen_buffer\r\n",
    "\r\n",
    "            # Stack the frames\r\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "         \r\n",
    "        else:\r\n",
    "            # If not done, the next_state become the current state\r\n",
    "            next_state = game.get_state().screen_buffer\r\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "            state = next_state\r\n",
    "                         \r\n",
    "    return np.stack(np.array(states)), np.stack(np.array(actions)), np.concatenate(rewards_of_batch), np.concatenate(discounted_rewards), episode_num"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Create the Neural Network\r\n",
    "* Initialize the weights\r\n",
    "* Init the environment\r\n",
    "* maxReward = 0 # Keep track of maximum reward\r\n",
    "* **For** epochs in range(num_epochs):\r\n",
    "    * Get batches\r\n",
    "    * Optimize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Keep track of all rewards total for each batch\r\n",
    "allRewards = []\r\n",
    "\r\n",
    "total_rewards = 0\r\n",
    "maximumRewardRecorded = 0\r\n",
    "mean_reward_total = []\r\n",
    "epoch = 1\r\n",
    "average_reward = []\r\n",
    "\r\n",
    "# Saver\r\n",
    "saver = tf.train.Saver()\r\n",
    "\r\n",
    "if training:\r\n",
    "    # Load the model\r\n",
    "    #saver.restore(sess, \"./models/model.ckpt\")\r\n",
    "\r\n",
    "    while epoch < num_epochs + 1:\r\n",
    "        # Gather training data\r\n",
    "        states_mb, actions_mb, rewards_of_batch, discounted_rewards_mb, nb_episodes_mb = make_batch(batch_size, stacked_frames)\r\n",
    "\r\n",
    "        ### These part is used for analytics\r\n",
    "        # Calculate the total reward ot the batch\r\n",
    "        total_reward_of_that_batch = np.sum(rewards_of_batch)\r\n",
    "        allRewards.append(total_reward_of_that_batch)\r\n",
    "\r\n",
    "        # Calculate the mean reward of the batch\r\n",
    "        # Total rewards of batch / nb episodes in that batch\r\n",
    "        mean_reward_of_that_batch = np.divide(total_reward_of_that_batch, nb_episodes_mb)\r\n",
    "        mean_reward_total.append(mean_reward_of_that_batch)\r\n",
    "\r\n",
    "        # Calculate the average reward of all training\r\n",
    "        # mean_reward_of_that_batch / epoch\r\n",
    "        average_reward_of_all_training = np.divide(np.sum(mean_reward_total), epoch)\r\n",
    "\r\n",
    "        # Calculate maximum reward recorded \r\n",
    "        maximumRewardRecorded = np.amax(allRewards)\r\n",
    "\r\n",
    "        print(\"==========================================\")\r\n",
    "        print(\"Epoch: \", epoch, \"/\", num_epochs)\r\n",
    "        print(\"-----------\")\r\n",
    "        print(\"Number of training episodes: {}\".format(nb_episodes_mb))\r\n",
    "        print(\"Total reward: {}\".format(total_reward_of_that_batch, nb_episodes_mb))\r\n",
    "        print(\"Mean Reward of that batch {}\".format(mean_reward_of_that_batch))\r\n",
    "        print(\"Average Reward of all training: {}\".format(average_reward_of_all_training))\r\n",
    "        print(\"Max reward for a batch so far: {}\".format(maximumRewardRecorded))\r\n",
    "\r\n",
    "        # Feedforward, gradient and backpropagation\r\n",
    "        loss_, _ = sess.run([PGNetwork.loss, PGNetwork.train_opt], feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\r\n",
    "                                                            PGNetwork.actions: actions_mb,\r\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb \r\n",
    "                                                                    })\r\n",
    "\r\n",
    "        print(\"Training Loss: {}\".format(loss_))\r\n",
    "\r\n",
    "        # Write TF Summaries\r\n",
    "        summary = sess.run(write_op, feed_dict={PGNetwork.inputs_: states_mb.reshape((len(states_mb), 84,84,4)),\r\n",
    "                                                            PGNetwork.actions: actions_mb,\r\n",
    "                                                                     PGNetwork.discounted_episode_rewards_: discounted_rewards_mb,\r\n",
    "                                                                    PGNetwork.mean_reward_: mean_reward_of_that_batch\r\n",
    "                                                                    })\r\n",
    "\r\n",
    "        #summary = sess.run(write_op, feed_dict={x: s_.reshape(len(s_),84,84,1), y:a_, d_r: d_r_, r: r_, n: n_})\r\n",
    "        writer.add_summary(summary, epoch)\r\n",
    "        writer.flush()\r\n",
    "\r\n",
    "        # Save Model\r\n",
    "        if epoch % 10 == 0:\r\n",
    "            saver.save(sess, \"./models/model.ckpt\")\r\n",
    "            print(\"Model saved\")\r\n",
    "        epoch += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "==========================================\n",
      "Epoch:  1 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 508.0\n",
      "Max reward for a batch so far: 1016.0\n",
      "Training Loss: -0.013492405414581299\n",
      "==========================================\n",
      "Epoch:  2 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 888.0\n",
      "Mean Reward of that batch 444.0\n",
      "Average Reward of all training: 476.0\n",
      "Max reward for a batch so far: 1016.0\n",
      "Training Loss: -0.004388260655105114\n",
      "==========================================\n",
      "Epoch:  3 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1848.0\n",
      "Mean Reward of that batch 924.0\n",
      "Average Reward of all training: 625.3333333333334\n",
      "Max reward for a batch so far: 1848.0\n",
      "Training Loss: -0.008172298781573772\n",
      "==========================================\n",
      "Epoch:  4 / 500\n",
      "-----------\n",
      "Number of training episodes: 3\n",
      "Total reward: 980.0\n",
      "Mean Reward of that batch 326.6666666666667\n",
      "Average Reward of all training: 550.6666666666666\n",
      "Max reward for a batch so far: 1848.0\n",
      "Training Loss: -0.009380221366882324\n",
      "==========================================\n",
      "Epoch:  5 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1048.0\n",
      "Mean Reward of that batch 524.0\n",
      "Average Reward of all training: 545.3333333333333\n",
      "Max reward for a batch so far: 1848.0\n",
      "Training Loss: -0.009533533826470375\n",
      "==========================================\n",
      "Epoch:  6 / 500\n",
      "-----------\n",
      "Number of training episodes: 2\n",
      "Total reward: 1016.0\n",
      "Mean Reward of that batch 508.0\n",
      "Average Reward of all training: 539.1111111111111\n",
      "Max reward for a batch so far: 1848.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Watch our Agent play ðŸ‘€\r\n",
    "Now that we trained our agent, we can test it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Saver\r\n",
    "saver = tf.train.Saver()\r\n",
    "\r\n",
    "with tf.Session() as sess:\r\n",
    "    game = DoomGame()\r\n",
    "\r\n",
    "    # Load the correct configuration \r\n",
    "    game.load_config(\"health_gathering.cfg\")\r\n",
    "    \r\n",
    "    # Load the correct scenario (in our case basic scenario)\r\n",
    "    game.set_doom_scenario_path(\"health_gathering.wad\")\r\n",
    "    \r\n",
    "    # Load the model\r\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\r\n",
    "    game.init()\r\n",
    "    \r\n",
    "    for i in range(10):\r\n",
    "        \r\n",
    "        # Launch a new episode\r\n",
    "        game.new_episode()\r\n",
    "\r\n",
    "        # Get a new state\r\n",
    "        state = game.get_state().screen_buffer\r\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "\r\n",
    "        while not game.is_episode_finished():\r\n",
    "        \r\n",
    "            # Run State Through Policy & Calculate Action\r\n",
    "            action_probability_distribution = sess.run(PGNetwork.action_distribution, \r\n",
    "                                                       feed_dict={PGNetwork.inputs_: state.reshape(1, *state_size)})\r\n",
    "\r\n",
    "            # REMEMBER THAT WE ARE IN A STOCHASTIC POLICY SO WE DON'T ALWAYS TAKE THE ACTION WITH THE HIGHEST PROBABILITY\r\n",
    "            # (For instance if the action with the best probability for state S is a1 with 70% chances, there is\r\n",
    "            #30% chance that we take action a2)\r\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), \r\n",
    "                                      p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\r\n",
    "            action = possible_actions[action]\r\n",
    "\r\n",
    "            # Perform action\r\n",
    "            reward = game.make_action(action)\r\n",
    "            done = game.is_episode_finished()\r\n",
    "            \r\n",
    "            if done:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                # If not done, the next_state become the current state\r\n",
    "                next_state = game.get_state().screen_buffer\r\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "                state = next_state\r\n",
    "        \r\n",
    "\r\n",
    "        print(\"Score for episode \", i, \" :\", game.get_total_reward())\r\n",
    "    game.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gameplai]",
   "language": "python",
   "name": "conda-env-gameplai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}