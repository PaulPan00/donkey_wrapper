{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Q learning with Doom üïπÔ∏è\n",
    "In this notebook we'll implement an agent <b>that plays Doom by using a Deep Q learning architecture.</b> <br>\n",
    "Our agent playing Doom:\n",
    "\n",
    "<img src=\"assets/doom.gif\" style=\"max-width: 600px;\" alt=\"Deep Q learning with Doom\"/>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/docs/assets/img/DRLC%20Environments.png\" alt=\"Deep Reinforcement Course\"/>\n",
    "<br>\n",
    "<p>  Deep Reinforcement Learning Course is a free series of articles and videos tutorials üÜï about Deep Reinforcement Learning, where **we'll learn the main algorithms (Q-learning, Deep Q Nets, Dueling Deep Q Nets, Policy Gradients, A2C, Proximal Policy Gradients‚Ä¶), and how to implement them with Tensorflow.**\n",
    "<br><br>\n",
    "    \n",
    "üìúThe articles explain the architectures from the big picture to the mathematical details behind them.\n",
    "<br>\n",
    "üìπ The videos explain how to build the agents with Tensorflow </b></p>\n",
    "<br>\n",
    "This course will give you a **solid foundation for understanding and implementing the future state of the art algorithms**. And, you'll build a strong professional portfolio by creating **agents that learn to play awesome environments**: Doom¬© üëπ, Space invaders üëæ, Outrun, Sonic the Hedgehog¬©, Michael Jackson‚Äôs Moonwalker, agents that will be able to navigate in 3D environments with DeepMindLab (Quake) and able to walk with Mujoco. \n",
    "<br><br>\n",
    "</p> \n",
    "\n",
    "## üìö The complete [Syllabus HERE](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "\n",
    "## Any questions üë®‚Äçüíª\n",
    "<p> If you have any questions, feel free to ask me: </p>\n",
    "<p> üìß: <a href=\"mailto:hello@simoninithomas.com\">hello@simoninithomas.com</a>  </p>\n",
    "<p> Github: https://github.com/simoninithomas/Deep_reinforcement_learning_Course </p>\n",
    "<p> üåê : https://simoninithomas.github.io/Deep_reinforcement_learning_Course/ </p>\n",
    "<p> Twitter: <a href=\"https://twitter.com/ThomasSimonini\">@ThomasSimonini</a> </p>\n",
    "<p> Don't forget to <b> follow me on <a href=\"https://twitter.com/ThomasSimonini\">twitter</a>, <a href=\"https://github.com/simoninithomas/Deep_reinforcement_learning_Course\">github</a> and <a href=\"https://medium.com/@thomassimonini\">Medium</a> to be alerted of the new articles that I publish </b></p>\n",
    "    \n",
    "## How to help  üôå\n",
    "3 ways:\n",
    "- **Clap our articles and like our videos a lot**:Clapping in Medium means that you really like our articles. And the more claps we have, the more our article is shared Liking our videos help them to be much more visible to the deep learning community.\n",
    "- **Share and speak about our articles and videos**: By sharing our articles and videos you help us to spread the word. \n",
    "- **Improve our notebooks**: if you found a bug or **a better implementation** you can send a pull request.\n",
    "<br>\n",
    "\n",
    "## Important note ü§î\n",
    "<b> You can run it on your computer but it's better to run it on GPU based services</b>, personally I use Microsoft Azure and their Deep Learning Virtual Machine (they offer 170$)\n",
    "https://azuremarketplace.microsoft.com/en-us/marketplace/apps/microsoft-ads.dsvm-deep-learning\n",
    "<br>\n",
    "‚ö†Ô∏è I don't have any business relations with them. I just loved their excellent customer service.\n",
    "\n",
    "If you have some troubles to use Microsoft Azure follow the explainations of this excellent article here (without last the part fast.ai): https://medium.com/@manikantayadunanda/setting-up-deeplearning-machine-and-fast-ai-on-azure-a22eb6bd6429"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prerequisites üèóÔ∏è\n",
    "Before diving on the notebook **you need to understand**:\n",
    "- The foundations of Reinforcement learning (MC, TD, Rewards hypothesis...) [Article](https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419)\n",
    "- Q-learning [Article](https://medium.freecodecamp.org/diving-deeper-into-reinforcement-learning-with-q-learning-c18d0db58efe)\n",
    "- Deep Q-Learning [Article](https://medium.freecodecamp.org/an-introduction-to-deep-q-learning-lets-play-doom-54d02d8017d8)\n",
    "- In the [video version](https://www.youtube.com/watch?v=gCJyVX98KJ4)  we implemented a Deep Q-learning agent with Tensorflow that learns to play Atari Space Invaders üïπÔ∏èüëæ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from IPython.display import HTML\r\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gCJyVX98KJ4?showinfo=0\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Import the libraries üìö"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf      # Deep Learning library\r\n",
    "import numpy as np           # Handle matrices\r\n",
    "from vizdoom import *        # Doom Environment\r\n",
    "\r\n",
    "import random                # Handling random number generation\r\n",
    "import time                  # Handling time calculation\r\n",
    "from skimage import transform# Help us to preprocess the frames\r\n",
    "\r\n",
    "from collections import deque# Ordered collection with ends\r\n",
    "import matplotlib.pyplot as plt # Display graphs\r\n",
    "\r\n",
    "import warnings # This ignore all the warning messages that are normally printed during the training because of skiimage\r\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Create our environment üéÆ\n",
    "- Now that we imported the libraries/dependencies, we will create our environment.\n",
    "- Doom environment takes:\n",
    "    - A `configuration file` that **handle all the options** (size of the frame, possible actions...)\n",
    "    - A `scenario file`: that **generates the correct scenario** (in our case basic **but you're invited to try other scenarios**).\n",
    "- Note: We have 3 possible actions `[[0,0,1], [1,0,0], [0,1,0]]` so we don't need to do one hot encoding (thanks to < a href=\"https://stackoverflow.com/users/2237916/silgon\">silgon</a> for figuring out. \n",
    "\n",
    "### Our environment\n",
    "<img src=\"assets/doom.png\" style=\"max-width:500px;\" alt=\"Doom\"/>\n",
    "                                    \n",
    "- A monster is spawned **randomly somewhere along the opposite wall**. \n",
    "- Player can only go **left/right and shoot**. \n",
    "- 1 hit is enough **to kill the monster**. \n",
    "- Episode finishes when **monster is killed or on timeout (300)**.\n",
    "<br><br>\n",
    "REWARDS:\n",
    "\n",
    "- +101 for killing the monster \n",
    "- -5 for missing \n",
    "- Episode ends after killing the monster or on timeout.\n",
    "- living reward = -1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"\"\"\r\n",
    "Here we create our environment\r\n",
    "\"\"\"\r\n",
    "def create_environment():\r\n",
    "    game = DoomGame()\r\n",
    "    \r\n",
    "    # Load the correct configuration\r\n",
    "    game.load_config(\"basic.cfg\")\r\n",
    "    \r\n",
    "    # Load the correct scenario (in our case basic scenario)\r\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\r\n",
    "    \r\n",
    "    game.init()\r\n",
    "    \r\n",
    "    # Here our possible actions\r\n",
    "    left = [1, 0, 0]\r\n",
    "    right = [0, 1, 0]\r\n",
    "    shoot = [0, 0, 1]\r\n",
    "    possible_actions = [left, right, shoot]\r\n",
    "    \r\n",
    "    return game, possible_actions\r\n",
    "       \r\n",
    "\"\"\"\r\n",
    "Here we performing random action to test the environment\r\n",
    "\"\"\"\r\n",
    "def test_environment():\r\n",
    "    game = DoomGame()\r\n",
    "    game.load_config(\"basic.cfg\")\r\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\r\n",
    "    game.init()\r\n",
    "    shoot = [0, 0, 1]\r\n",
    "    left = [1, 0, 0]\r\n",
    "    right = [0, 1, 0]\r\n",
    "    actions = [shoot, left, right]\r\n",
    "\r\n",
    "    episodes = 10\r\n",
    "    for i in range(episodes):\r\n",
    "        game.new_episode()\r\n",
    "        while not game.is_episode_finished():\r\n",
    "            state = game.get_state()\r\n",
    "            img = state.screen_buffer\r\n",
    "            misc = state.game_variables\r\n",
    "            action = random.choice(actions)\r\n",
    "            print(action)\r\n",
    "            reward = game.make_action(action)\r\n",
    "            print (\"\\treward:\", reward)\r\n",
    "            time.sleep(0.02)\r\n",
    "        print (\"Result:\", game.get_total_reward())\r\n",
    "        time.sleep(2)\r\n",
    "    game.close()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "game, possible_actions = create_environment()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define the preprocessing functions ‚öôÔ∏è\n",
    "### preprocess_frame\n",
    "Preprocessing is an important step, <b>because we want to reduce the complexity of our states to reduce the computation time needed for training.</b>\n",
    "<br><br>\n",
    "Our steps:\n",
    "- Grayscale each of our frames (because <b> color does not add important information </b>). But this is already done by the config file.\n",
    "- Crop the screen (in our case we remove the roof because it contains no information)\n",
    "- We normalize pixel values\n",
    "- Finally we resize the preprocessed frame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\"\"\"\r\n",
    "    preprocess_frame:\r\n",
    "    Take a frame.\r\n",
    "    Resize it.\r\n",
    "        __________________\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |                 |\r\n",
    "        |_________________|\r\n",
    "        \r\n",
    "        to\r\n",
    "        _____________\r\n",
    "        |            |\r\n",
    "        |            |\r\n",
    "        |            |\r\n",
    "        |____________|\r\n",
    "    Normalize it.\r\n",
    "    \r\n",
    "    return preprocessed_frame\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "def preprocess_frame(frame):\r\n",
    "    # Greyscale frame already done in our vizdoom config\r\n",
    "    # x = np.mean(frame,-1)\r\n",
    "    \r\n",
    "    # Crop the screen (remove the roof because it contains no information)\r\n",
    "    cropped_frame = frame[30:-10, 30:-30]\r\n",
    "    \r\n",
    "    # Normalize Pixel Values\r\n",
    "    normalized_frame = cropped_frame / 255.0\r\n",
    "    \r\n",
    "    # Resize\r\n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\r\n",
    "    \r\n",
    "    return preprocessed_frame"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### stack_frames\n",
    "üëè This part was made possible thanks to help of <a href=\"https://github.com/Miffyli\">Anssi</a><br>\n",
    "\n",
    "As explained in this really <a href=\"https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\">  good article </a> we stack frames.\n",
    "\n",
    "Stacking frames is really important because it helps us to **give have a sense of motion to our Neural Network.**\n",
    "\n",
    "- First we preprocess frame\n",
    "- Then we append the frame to the deque that automatically **removes the oldest frame**\n",
    "- Finally we **build the stacked state**\n",
    "\n",
    "This is how work stack:\n",
    "- For the first frame, we feed 4 frames\n",
    "- At each timestep, **we add the new frame to deque and then we stack them to form a new stacked frame**\n",
    "- And so on\n",
    "<img src=\"https://raw.githubusercontent.com/simoninithomas/Deep_reinforcement_learning_Course/master/DQN/Space%20Invaders/assets/stack_frames.png\" alt=\"stack\">\n",
    "- If we're done, **we create a new stack with 4 new frames (because we are in a new episode)**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "stack_size = 4 # We stack 4 frames\r\n",
    "\r\n",
    "# Initialize deque with zero-images one array for each image\r\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen=4) \r\n",
    "\r\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\r\n",
    "    # Preprocess frame\r\n",
    "    frame = preprocess_frame(state)\r\n",
    "    \r\n",
    "    if is_new_episode:\r\n",
    "        # Clear our stacked_frames\r\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype=np.int) for i in range(stack_size)], maxlen=4)\r\n",
    "        \r\n",
    "        # Because we're in a new episode, copy the same frame 4x\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        stacked_frames.append(frame)\r\n",
    "        \r\n",
    "        # Stack the frames\r\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\r\n",
    "    else:\r\n",
    "        # Append frame to deque, automatically removes the oldest frame\r\n",
    "        stacked_frames.append(frame)\r\n",
    "\r\n",
    "        # Build the stacked state (first dimension specifies different frames)\r\n",
    "        stacked_state = np.stack(stacked_frames, axis=2) \r\n",
    "    \r\n",
    "    return stacked_state, stacked_frames"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Set up our hyperparameters ‚öóÔ∏è\n",
    "In this part we'll set up our different hyperparameters. But when you implement a Neural Network by yourself you will **not implement hyperparamaters at once but progressively**.\n",
    "\n",
    "- First, you begin by defining the neural networks hyperparameters when you implement the model.\n",
    "- Then, you'll add the training hyperparameters when you implement the training algorithm."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "### MODEL HYPERPARAMETERS\r\n",
    "state_size = [84, 84, 4]      # Our input is a stack of 4 frames hence 84x84x4 (Width, height, channels) \r\n",
    "action_size = game.get_available_buttons_size()              # 3 possible actions: left, right, shoot\r\n",
    "learning_rate =  0.0002      # Alpha (aka learning rate)\r\n",
    "\r\n",
    "### TRAINING HYPERPARAMETERS\r\n",
    "total_episodes = 500        # Total episodes for training\r\n",
    "max_steps = 100              # Max possible steps in an episode\r\n",
    "batch_size = 64             \r\n",
    "\r\n",
    "# Exploration parameters for epsilon greedy strategy\r\n",
    "explore_start = 1.0            # exploration probability at start\r\n",
    "explore_stop = 0.01            # minimum exploration probability \r\n",
    "decay_rate = 0.0001            # exponential decay rate for exploration prob\r\n",
    "\r\n",
    "# Q learning hyperparameters\r\n",
    "gamma = 0.95               # Discounting rate\r\n",
    "\r\n",
    "### MEMORY HYPERPARAMETERS\r\n",
    "pretrain_length = batch_size   # Number of experiences stored in the Memory when initialized for the first time\r\n",
    "memory_size = 1000000          # Number of experiences the Memory can keep\r\n",
    "\r\n",
    "### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT\r\n",
    "training = True\r\n",
    "\r\n",
    "## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT\r\n",
    "episode_render = False"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 5: Create our Deep Q-learning Neural Network model üß†\n",
    "<img src=\"assets/model.png\" alt=\"Model\" />\n",
    "This is our Deep Q-learning model:\n",
    "- We take a stack of 4 frames as input\n",
    "- It passes through 3 convnets\n",
    "- Then it is flatened\n",
    "- Finally it passes through 2 FC layers\n",
    "- It outputs a Q value for each actions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class DQNetwork:\r\n",
    "    def __init__(self, state_size, action_size, learning_rate, name='DQNetwork'):\r\n",
    "        self.state_size = state_size\r\n",
    "        self.action_size = action_size\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        \r\n",
    "        with tf.variable_scope(name):\r\n",
    "            # We create the placeholders\r\n",
    "            # *state_size means that we take each elements of state_size in tuple hence is like if we wrote\r\n",
    "            # [None, 84, 84, 4]\r\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name=\"inputs\")\r\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name=\"actions_\")\r\n",
    "            \r\n",
    "            # Remember that target_Q is the R(s,a) + ymax Qhat(s', a')\r\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name=\"target\")\r\n",
    "            \r\n",
    "            \"\"\"\r\n",
    "            First convnet:\r\n",
    "            CNN\r\n",
    "            BatchNormalization\r\n",
    "            ELU\r\n",
    "            \"\"\"\r\n",
    "            # Input is 84x84x4\r\n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\r\n",
    "                                         filters = 32,\r\n",
    "                                         kernel_size = [8,8],\r\n",
    "                                         strides = [4,4],\r\n",
    "                                         padding = \"VALID\",\r\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                         name = \"conv1\")\r\n",
    "            \r\n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\r\n",
    "                                                   training = True,\r\n",
    "                                                   epsilon = 1e-5,\r\n",
    "                                                     name = 'batch_norm1')\r\n",
    "            \r\n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\r\n",
    "            ## --> [20, 20, 32]\r\n",
    "            \r\n",
    "            \r\n",
    "            \"\"\"\r\n",
    "            Second convnet:\r\n",
    "            CNN\r\n",
    "            BatchNormalization\r\n",
    "            ELU\r\n",
    "            \"\"\"\r\n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\r\n",
    "                                 filters = 64,\r\n",
    "                                 kernel_size = [4,4],\r\n",
    "                                 strides = [2,2],\r\n",
    "                                 padding = \"VALID\",\r\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                 name = \"conv2\")\r\n",
    "        \r\n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\r\n",
    "                                                   training = True,\r\n",
    "                                                   epsilon = 1e-5,\r\n",
    "                                                     name = 'batch_norm2')\r\n",
    "\r\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\r\n",
    "            ## --> [9, 9, 64]\r\n",
    "            \r\n",
    "            \r\n",
    "            \"\"\"\r\n",
    "            Third convnet:\r\n",
    "            CNN\r\n",
    "            BatchNormalization\r\n",
    "            ELU\r\n",
    "            \"\"\"\r\n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\r\n",
    "                                 filters = 128,\r\n",
    "                                 kernel_size = [4,4],\r\n",
    "                                 strides = [2,2],\r\n",
    "                                 padding = \"VALID\",\r\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\r\n",
    "                                 name = \"conv3\")\r\n",
    "        \r\n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\r\n",
    "                                                   training = True,\r\n",
    "                                                   epsilon = 1e-5,\r\n",
    "                                                     name = 'batch_norm3')\r\n",
    "\r\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\r\n",
    "            ## --> [3, 3, 128]\r\n",
    "            \r\n",
    "            \r\n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\r\n",
    "            ## --> [1152]\r\n",
    "            \r\n",
    "            \r\n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\r\n",
    "                                  units = 512,\r\n",
    "                                  activation = tf.nn.elu,\r\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
    "                                name=\"fc1\")\r\n",
    "            \r\n",
    "            \r\n",
    "            self.output = tf.layers.dense(inputs = self.fc, \r\n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\r\n",
    "                                          units = 3, \r\n",
    "                                        activation=None)\r\n",
    "\r\n",
    "  \r\n",
    "            # Q is our predicted Q value.\r\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\r\n",
    "            \r\n",
    "            \r\n",
    "            # The loss is the difference between our predicted Q_values and the Q_target\r\n",
    "            # Sum(Qtarget - Q)^2\r\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\r\n",
    "            \r\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Reset the graph\r\n",
    "# tf.reset_default_graph()\r\n",
    "\r\n",
    "# Instantiate the DQNetwork\r\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-24f0f99070ea>:30: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\envs\\python_3.7.4\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-24f0f99070ea>:35: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-10-24f0f99070ea>:87: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From <ipython-input-10-24f0f99070ea>:95: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\envs\\python_3.7.4\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 6: Experience Replay üîÅ\n",
    "Now that we create our Neural Network, **we need to implement the Experience Replay method.** <br><br>\n",
    "Here we'll create the Memory object that creates a deque.A deque (double ended queue) is a data type that **removes the oldest element each time that you add a new element.**\n",
    "\n",
    "This part was taken from Udacity : <a href=\"https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb\" Cartpole DQN</a>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class Memory():\r\n",
    "    def __init__(self, max_size):\r\n",
    "        self.buffer = deque(maxlen = max_size)\r\n",
    "    \r\n",
    "    def add(self, experience):\r\n",
    "        self.buffer.append(experience)\r\n",
    "    \r\n",
    "    def sample(self, batch_size):\r\n",
    "        buffer_size = len(self.buffer)\r\n",
    "        index = np.random.choice(np.arange(buffer_size),\r\n",
    "                                size = batch_size,\r\n",
    "                                replace = False)\r\n",
    "        \r\n",
    "        return [self.buffer[i] for i in index]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we'll **deal with the empty memory problem**: we pre-populate our memory by taking random actions and storing the experience (state, action, reward, new_state)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# Instantiate memory\r\n",
    "memory = Memory(max_size = memory_size)\r\n",
    "\r\n",
    "# Render the environment\r\n",
    "game.new_episode()\r\n",
    "\r\n",
    "for i in range(pretrain_length):\r\n",
    "    # If it's the first step\r\n",
    "    if i == 0:\r\n",
    "        # First we need a state\r\n",
    "        state = game.get_state().screen_buffer\r\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "    \r\n",
    "    # Random action\r\n",
    "    action = random.choice(possible_actions)\r\n",
    "    \r\n",
    "    # Get the rewards\r\n",
    "    reward = game.make_action(action)\r\n",
    "    \r\n",
    "    # Look if the episode is finished\r\n",
    "    done = game.is_episode_finished()\r\n",
    "    \r\n",
    "    # If we're dead\r\n",
    "    if done:\r\n",
    "        # We finished the episode\r\n",
    "        next_state = np.zeros(state.shape)\r\n",
    "        \r\n",
    "        # Add experience to memory\r\n",
    "        memory.add((state, action, reward, next_state, done))\r\n",
    "        \r\n",
    "        # Start a new episode\r\n",
    "        game.new_episode()\r\n",
    "        \r\n",
    "        # First we need a state\r\n",
    "        state = game.get_state().screen_buffer\r\n",
    "        \r\n",
    "        # Stack the frames\r\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "        \r\n",
    "    else:\r\n",
    "        # Get the next state\r\n",
    "        next_state = game.get_state().screen_buffer\r\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "        \r\n",
    "        # Add experience to memory\r\n",
    "        memory.add((state, action, reward, next_state, done))\r\n",
    "        \r\n",
    "        # Our state is now the next_state\r\n",
    "        state = next_state"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 7: Set up Tensorboard üìä\r\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\r\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/dqn/1`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Setup TensorBoard Writer\r\n",
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\r\n",
    "\r\n",
    "## Losses\r\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\r\n",
    "\r\n",
    "write_op = tf.summary.merge_all()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "!tensorboard --logdir=/tensorboard/dqn/1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 8: Train our Agent üèÉ‚Äç‚ôÇÔ∏è\n",
    "\n",
    "Our algorithm:\n",
    "<br>\n",
    "* Initialize the weights\n",
    "* Init the environment\n",
    "* Initialize the decay rate (that will use to reduce epsilon) \n",
    "<br><br>\n",
    "* **For** episode to max_episode **do** \n",
    "    * Make new episode\n",
    "    * Set step to 0\n",
    "    * Observe the first state $s_0$\n",
    "    <br><br>\n",
    "    * **While** step < max_steps **do**:\n",
    "        * Increase decay_rate\n",
    "        * With $\\epsilon$ select a random action $a_t$, otherwise select $a_t = \\mathrm{argmax}_a Q(s_t,a)$\n",
    "        * Execute action $a_t$ in simulator and observe reward $r_{t+1}$ and new state $s_{t+1}$\n",
    "        * Store transition $<s_t, a_t, r_{t+1}, s_{t+1}>$ in memory $D$\n",
    "        * Sample random mini-batch from $D$: $<s, a, r, s'>$\n",
    "        * Set $\\hat{Q} = r$ if the episode ends at $+1$, otherwise set $\\hat{Q} = r + \\gamma \\max_{a'}{Q(s', a')}$\n",
    "        * Make a gradient descent step with loss $(\\hat{Q} - Q(s, a))^2$\n",
    "    * **endfor**\n",
    "    <br><br>\n",
    "* **endfor**\n",
    "\n",
    "    "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\"\"\"\r\n",
    "This function will do the part\r\n",
    "With œµ select a random action atat, otherwise select at=argmaxaQ(st,a)\r\n",
    "\"\"\"\r\n",
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\r\n",
    "    ## EPSILON GREEDY STRATEGY\r\n",
    "    # Choose action a from state s using epsilon greedy.\r\n",
    "    ## First we randomize a number\r\n",
    "    exp_exp_tradeoff = np.random.rand()\r\n",
    "\r\n",
    "    # Here we'll use an improved version of our epsilon greedy strategy used in Q-learning notebook\r\n",
    "    explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)\r\n",
    "    \r\n",
    "    if (explore_probability > exp_exp_tradeoff):\r\n",
    "        # Make a random action (exploration)\r\n",
    "        action = random.choice(possible_actions)\r\n",
    "        \r\n",
    "    else:\r\n",
    "        # Get action from Q-network (exploitation)\r\n",
    "        # Estimate the Qs values state\r\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\r\n",
    "        \r\n",
    "        # Take the biggest Q value (= the best action)\r\n",
    "        choice = np.argmax(Qs)\r\n",
    "        action = possible_actions[int(choice)]\r\n",
    "                \r\n",
    "    return action, explore_probability"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Saver will help us to save our model\r\n",
    "saver = tf.train.Saver()\r\n",
    "\r\n",
    "if training == True:\r\n",
    "    with tf.Session() as sess:\r\n",
    "        # Initialize the variables\r\n",
    "        sess.run(tf.global_variables_initializer())\r\n",
    "        \r\n",
    "        # Initialize the decay rate (that will use to reduce epsilon) \r\n",
    "        decay_step = 0\r\n",
    "\r\n",
    "        # Init the game\r\n",
    "        game.init()\r\n",
    "\r\n",
    "        for episode in range(total_episodes):\r\n",
    "            # Set step to 0\r\n",
    "            step = 0\r\n",
    "            \r\n",
    "            # Initialize the rewards of the episode\r\n",
    "            episode_rewards = []\r\n",
    "            \r\n",
    "            # Make a new episode and observe the first state\r\n",
    "            game.new_episode()\r\n",
    "            state = game.get_state().screen_buffer\r\n",
    "            \r\n",
    "            # Remember that stack frame function also call our preprocess function.\r\n",
    "            state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "\r\n",
    "            while step < max_steps:\r\n",
    "                step += 1\r\n",
    "                \r\n",
    "                # Increase decay_step\r\n",
    "                decay_step +=1\r\n",
    "                \r\n",
    "                # Predict the action to take and take it\r\n",
    "                action, explore_probability = predict_action(explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)\r\n",
    "\r\n",
    "                # Do the action\r\n",
    "                reward = game.make_action(action)\r\n",
    "\r\n",
    "                # Look if the episode is finished\r\n",
    "                done = game.is_episode_finished()\r\n",
    "                \r\n",
    "                # Add the reward to total reward\r\n",
    "                episode_rewards.append(reward)\r\n",
    "\r\n",
    "                # If the game is finished\r\n",
    "                if done:\r\n",
    "                    # the episode ends so no next state\r\n",
    "                    next_state = np.zeros((84,84), dtype=np.int)\r\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "\r\n",
    "                    # Set step = max_steps to end the episode\r\n",
    "                    step = max_steps\r\n",
    "\r\n",
    "                    # Get the total reward of the episode\r\n",
    "                    total_reward = np.sum(episode_rewards)\r\n",
    "\r\n",
    "                    print('Episode: {}'.format(episode),\r\n",
    "                              'Total reward: {}'.format(total_reward),\r\n",
    "                              'Training loss: {:.4f}'.format(loss),\r\n",
    "                              'Explore P: {:.4f}'.format(explore_probability))\r\n",
    "\r\n",
    "                    memory.add((state, action, reward, next_state, done))\r\n",
    "\r\n",
    "                else:\r\n",
    "                    # Get the next state\r\n",
    "                    next_state = game.get_state().screen_buffer\r\n",
    "                    \r\n",
    "                    # Stack the frame of the next_state\r\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "                    \r\n",
    "\r\n",
    "                    # Add experience to memory\r\n",
    "                    memory.add((state, action, reward, next_state, done))\r\n",
    "                    \r\n",
    "                    # st+1 is now our current state\r\n",
    "                    state = next_state\r\n",
    "\r\n",
    "\r\n",
    "                ### LEARNING PART            \r\n",
    "                # Obtain random mini-batch from memory\r\n",
    "                batch = memory.sample(batch_size)\r\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin=3)\r\n",
    "                actions_mb = np.array([each[1] for each in batch])\r\n",
    "                rewards_mb = np.array([each[2] for each in batch]) \r\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin=3)\r\n",
    "                dones_mb = np.array([each[4] for each in batch])\r\n",
    "\r\n",
    "                target_Qs_batch = []\r\n",
    "\r\n",
    "                 # Get Q values for next_state \r\n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\r\n",
    "                \r\n",
    "                # Set Q_target = r if the episode ends at s+1, otherwise set Q_target = r + gamma*maxQ(s', a')\r\n",
    "                for i in range(0, len(batch)):\r\n",
    "                    terminal = dones_mb[i]\r\n",
    "\r\n",
    "                    # If we are in a terminal state, only equals reward\r\n",
    "                    if terminal:\r\n",
    "                        target_Qs_batch.append(rewards_mb[i])\r\n",
    "                        \r\n",
    "                    else:\r\n",
    "                        target = rewards_mb[i] + gamma * np.max(Qs_next_state[i])\r\n",
    "                        target_Qs_batch.append(target)\r\n",
    "                        \r\n",
    "\r\n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\r\n",
    "\r\n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\r\n",
    "                                    feed_dict={DQNetwork.inputs_: states_mb,\r\n",
    "                                               DQNetwork.target_Q: targets_mb,\r\n",
    "                                               DQNetwork.actions_: actions_mb})\r\n",
    "\r\n",
    "                # Write TF Summaries\r\n",
    "                summary = sess.run(write_op, feed_dict={DQNetwork.inputs_: states_mb,\r\n",
    "                                                   DQNetwork.target_Q: targets_mb,\r\n",
    "                                                   DQNetwork.actions_: actions_mb})\r\n",
    "                writer.add_summary(summary, episode)\r\n",
    "                writer.flush()\r\n",
    "\r\n",
    "            # Save model every 5 episodes\r\n",
    "            if episode % 5 == 0:\r\n",
    "                save_path = saver.save(sess, \"./models/model.ckpt\")\r\n",
    "                print(\"Model Saved\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 0 Total reward: 67.0 Training loss: 1.0470 Explore P: 0.9971\n",
      "Model Saved\n",
      "Episode: 3 Total reward: 95.0 Training loss: 1.4169 Explore P: 0.9770\n",
      "Episode: 5 Total reward: 94.0 Training loss: 4.2568 Explore P: 0.9667\n",
      "Model Saved\n",
      "Episode: 6 Total reward: 95.0 Training loss: 8.6831 Explore P: 0.9661\n",
      "Episode: 7 Total reward: 95.0 Training loss: 19.1226 Explore P: 0.9656\n",
      "Episode: 10 Total reward: 94.0 Training loss: 1.2128 Explore P: 0.9460\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 95.0 Training loss: 4.7878 Explore P: 0.9454\n",
      "Episode: 12 Total reward: 95.0 Training loss: 4.3903 Explore P: 0.9449\n",
      "Episode: 13 Total reward: 91.0 Training loss: 4.0964 Explore P: 0.9439\n",
      "Episode: 15 Total reward: 95.0 Training loss: 1.5670 Explore P: 0.9341\n",
      "Model Saved\n",
      "Episode: 17 Total reward: 92.0 Training loss: 0.8014 Explore P: 0.9241\n",
      "Episode: 18 Total reward: 17.0 Training loss: 8.3715 Explore P: 0.9178\n",
      "Episode: 19 Total reward: 7.0 Training loss: 8.2944 Explore P: 0.9111\n",
      "Episode: 20 Total reward: 95.0 Training loss: 68.3635 Explore P: 0.9105\n",
      "Model Saved\n",
      "Episode: 21 Total reward: 22.0 Training loss: 14.0980 Explore P: 0.9048\n",
      "Episode: 22 Total reward: 57.0 Training loss: 9.6928 Explore P: 0.9018\n",
      "Episode: 24 Total reward: 94.0 Training loss: 7.9901 Explore P: 0.8923\n",
      "Model Saved\n",
      "Episode: 26 Total reward: 75.0 Training loss: 12.4202 Explore P: 0.8817\n",
      "Episode: 27 Total reward: 94.0 Training loss: 2.9862 Explore P: 0.8811\n",
      "Episode: 28 Total reward: 92.0 Training loss: 10.9161 Explore P: 0.8803\n",
      "Episode: 30 Total reward: 89.0 Training loss: 3.1156 Explore P: 0.8706\n",
      "Model Saved\n",
      "Episode: 31 Total reward: 70.0 Training loss: 6.4539 Explore P: 0.8683\n",
      "Episode: 32 Total reward: 70.0 Training loss: 6.9091 Explore P: 0.8661\n",
      "Episode: 33 Total reward: 95.0 Training loss: 7.8032 Explore P: 0.8656\n",
      "Episode: 35 Total reward: 89.0 Training loss: 17.9251 Explore P: 0.8561\n",
      "Model Saved\n",
      "Episode: 37 Total reward: 93.0 Training loss: 15.5984 Explore P: 0.8470\n",
      "Episode: 38 Total reward: 91.0 Training loss: 4.0658 Explore P: 0.8461\n",
      "Episode: 40 Total reward: 86.0 Training loss: 5.9232 Explore P: 0.8366\n",
      "Model Saved\n",
      "Episode: 41 Total reward: 95.0 Training loss: 5.4902 Explore P: 0.8361\n",
      "Episode: 42 Total reward: 67.0 Training loss: 5.2741 Explore P: 0.8337\n",
      "Model Saved\n",
      "Episode: 46 Total reward: 93.0 Training loss: 15.1568 Explore P: 0.8087\n",
      "Episode: 47 Total reward: -1.0 Training loss: 7.6571 Explore P: 0.8022\n",
      "Episode: 49 Total reward: 61.0 Training loss: 9.8291 Explore P: 0.7916\n",
      "Episode: 50 Total reward: 93.0 Training loss: 5.1462 Explore P: 0.7909\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 68.0 Training loss: 6.9957 Explore P: 0.7888\n",
      "Episode: 52 Total reward: 95.0 Training loss: 4.0570 Explore P: 0.7883\n",
      "Episode: 53 Total reward: 70.0 Training loss: 4.6686 Explore P: 0.7863\n",
      "Episode: 54 Total reward: 93.0 Training loss: 35.0223 Explore P: 0.7857\n",
      "Episode: 55 Total reward: 94.0 Training loss: 5.1175 Explore P: 0.7851\n",
      "Model Saved\n",
      "Episode: 56 Total reward: 95.0 Training loss: 8.8173 Explore P: 0.7846\n",
      "Episode: 57 Total reward: 95.0 Training loss: 6.0636 Explore P: 0.7842\n",
      "Episode: 58 Total reward: 95.0 Training loss: 7.3905 Explore P: 0.7837\n",
      "Episode: 59 Total reward: 95.0 Training loss: 13.5766 Explore P: 0.7833\n",
      "Episode: 60 Total reward: 93.0 Training loss: 3.3526 Explore P: 0.7826\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 93.0 Training loss: 7.0056 Explore P: 0.7820\n",
      "Episode: 62 Total reward: 95.0 Training loss: 6.7185 Explore P: 0.7816\n",
      "Episode: 63 Total reward: 75.0 Training loss: 8.4884 Explore P: 0.7799\n",
      "Episode: 64 Total reward: 95.0 Training loss: 15.2424 Explore P: 0.7795\n",
      "Model Saved\n",
      "Episode: 66 Total reward: 15.0 Training loss: 4.3537 Explore P: 0.7664\n",
      "Episode: 68 Total reward: 93.0 Training loss: 17.3211 Explore P: 0.7583\n",
      "Episode: 69 Total reward: 95.0 Training loss: 7.6926 Explore P: 0.7579\n",
      "Episode: 70 Total reward: 95.0 Training loss: 5.5006 Explore P: 0.7574\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 95.0 Training loss: 5.8952 Explore P: 0.7570\n",
      "Episode: 72 Total reward: 95.0 Training loss: 12.6244 Explore P: 0.7565\n",
      "Episode: 73 Total reward: 94.0 Training loss: 19.9322 Explore P: 0.7560\n",
      "Episode: 74 Total reward: 95.0 Training loss: 5.5838 Explore P: 0.7555\n",
      "Model Saved\n",
      "Episode: 76 Total reward: 94.0 Training loss: 6.0367 Explore P: 0.7476\n",
      "Episode: 77 Total reward: 39.0 Training loss: 6.1296 Explore P: 0.7438\n",
      "Episode: 78 Total reward: 72.0 Training loss: 4.9757 Explore P: 0.7420\n",
      "Episode: 79 Total reward: 42.0 Training loss: 12.9289 Explore P: 0.7384\n",
      "Episode: 80 Total reward: 95.0 Training loss: 5.8628 Explore P: 0.7380\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 95.0 Training loss: 7.3286 Explore P: 0.7376\n",
      "Episode: 82 Total reward: 32.0 Training loss: 9.9085 Explore P: 0.7333\n",
      "Episode: 83 Total reward: 93.0 Training loss: 16.4866 Explore P: 0.7327\n",
      "Episode: 84 Total reward: 88.0 Training loss: 9.2080 Explore P: 0.7318\n",
      "Episode: 85 Total reward: 93.0 Training loss: 7.4303 Explore P: 0.7312\n",
      "Model Saved\n",
      "Episode: 86 Total reward: 94.0 Training loss: 5.8300 Explore P: 0.7307\n",
      "Episode: 87 Total reward: 95.0 Training loss: 76.6679 Explore P: 0.7303\n",
      "Episode: 90 Total reward: 95.0 Training loss: 3.8223 Explore P: 0.7156\n",
      "Model Saved\n",
      "Episode: 93 Total reward: 95.0 Training loss: 10.5011 Explore P: 0.7012\n",
      "Model Saved\n",
      "Episode: 97 Total reward: -1.0 Training loss: 9.5406 Explore P: 0.6753\n",
      "Episode: 98 Total reward: 95.0 Training loss: 9.5706 Explore P: 0.6749\n",
      "Episode: 99 Total reward: 17.0 Training loss: 13.9971 Explore P: 0.6703\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 12.0 Training loss: 8.9545 Explore P: 0.6589\n",
      "Episode: 102 Total reward: 95.0 Training loss: 5.1769 Explore P: 0.6585\n",
      "Episode: 104 Total reward: 95.0 Training loss: 10.0329 Explore P: 0.6517\n",
      "Episode: 105 Total reward: -2.0 Training loss: 10.2597 Explore P: 0.6464\n",
      "Model Saved\n",
      "Episode: 106 Total reward: 95.0 Training loss: 7.6344 Explore P: 0.6460\n",
      "Episode: 107 Total reward: 93.0 Training loss: 7.1491 Explore P: 0.6455\n",
      "Episode: 108 Total reward: -9.0 Training loss: 5.3065 Explore P: 0.6398\n",
      "Episode: 109 Total reward: 14.0 Training loss: 6.2435 Explore P: 0.6353\n",
      "Episode: 110 Total reward: 94.0 Training loss: 3.3438 Explore P: 0.6348\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 95.0 Training loss: 8.9315 Explore P: 0.6345\n",
      "Episode: 112 Total reward: 95.0 Training loss: 5.4676 Explore P: 0.6341\n",
      "Episode: 114 Total reward: 69.0 Training loss: 5.6337 Explore P: 0.6262\n",
      "Episode: 115 Total reward: 28.0 Training loss: 6.5177 Explore P: 0.6227\n",
      "Model Saved\n",
      "Episode: 116 Total reward: 2.0 Training loss: 15.2997 Explore P: 0.6175\n",
      "Episode: 117 Total reward: 95.0 Training loss: 7.2096 Explore P: 0.6172\n",
      "Episode: 118 Total reward: 68.0 Training loss: 5.2490 Explore P: 0.6155\n",
      "Episode: 119 Total reward: 95.0 Training loss: 8.3633 Explore P: 0.6151\n",
      "Episode: 120 Total reward: 15.0 Training loss: 7.5095 Explore P: 0.6108\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 41.0 Training loss: 4.4882 Explore P: 0.6078\n",
      "Episode: 122 Total reward: 95.0 Training loss: 6.7449 Explore P: 0.6075\n",
      "Episode: 123 Total reward: 76.0 Training loss: 3.7798 Explore P: 0.6063\n",
      "Episode: 124 Total reward: 95.0 Training loss: 9.2780 Explore P: 0.6059\n",
      "Episode: 125 Total reward: 94.0 Training loss: 10.7110 Explore P: 0.6055\n",
      "Model Saved\n",
      "Episode: 126 Total reward: 95.0 Training loss: 11.6103 Explore P: 0.6051\n",
      "Episode: 127 Total reward: 93.0 Training loss: 11.7001 Explore P: 0.6047\n",
      "Episode: 128 Total reward: 94.0 Training loss: 9.5795 Explore P: 0.6043\n",
      "Episode: 129 Total reward: 95.0 Training loss: 18.2652 Explore P: 0.6039\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 95.0 Training loss: 7.8107 Explore P: 0.5976\n",
      "Episode: 133 Total reward: 75.0 Training loss: 6.8737 Explore P: 0.5906\n",
      "Episode: 134 Total reward: 95.0 Training loss: 11.2554 Explore P: 0.5902\n",
      "Episode: 135 Total reward: 95.0 Training loss: 7.3393 Explore P: 0.5899\n",
      "Model Saved\n",
      "Episode: 136 Total reward: 76.0 Training loss: 17.0768 Explore P: 0.5887\n",
      "Episode: 137 Total reward: 94.0 Training loss: 5.7765 Explore P: 0.5883\n",
      "Episode: 138 Total reward: 92.0 Training loss: 7.8494 Explore P: 0.5878\n",
      "Episode: 139 Total reward: 8.0 Training loss: 5.4285 Explore P: 0.5836\n",
      "Episode: 140 Total reward: 95.0 Training loss: 19.1841 Explore P: 0.5832\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 71.0 Training loss: 7.1564 Explore P: 0.5818\n",
      "Episode: 143 Total reward: 92.0 Training loss: 5.7148 Explore P: 0.5756\n",
      "Episode: 144 Total reward: 95.0 Training loss: 6.0192 Explore P: 0.5753\n",
      "Episode: 145 Total reward: 31.0 Training loss: 10.3960 Explore P: 0.5719\n",
      "Model Saved\n",
      "Episode: 146 Total reward: 76.0 Training loss: 6.8995 Explore P: 0.5708\n",
      "Episode: 147 Total reward: 95.0 Training loss: 11.0561 Explore P: 0.5704\n",
      "Episode: 148 Total reward: 75.0 Training loss: 6.3253 Explore P: 0.5693\n",
      "Episode: 150 Total reward: 95.0 Training loss: 13.9398 Explore P: 0.5634\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 93.0 Training loss: 5.6290 Explore P: 0.5629\n",
      "Episode: 152 Total reward: 67.0 Training loss: 4.8774 Explore P: 0.5613\n",
      "Episode: 153 Total reward: 95.0 Training loss: 6.2696 Explore P: 0.5610\n",
      "Episode: 154 Total reward: 94.0 Training loss: 8.1263 Explore P: 0.5606\n",
      "Model Saved\n",
      "Episode: 156 Total reward: 94.0 Training loss: 7.1067 Explore P: 0.5547\n",
      "Episode: 158 Total reward: 95.0 Training loss: 6.7367 Explore P: 0.5490\n",
      "Episode: 159 Total reward: 71.0 Training loss: 9.2965 Explore P: 0.5476\n",
      "Episode: 160 Total reward: 92.0 Training loss: 4.2130 Explore P: 0.5472\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 91.0 Training loss: 8.3934 Explore P: 0.5466\n",
      "Episode: 162 Total reward: 48.0 Training loss: 7.7519 Explore P: 0.5443\n",
      "Episode: 163 Total reward: 95.0 Training loss: 5.0956 Explore P: 0.5440\n",
      "Episode: 164 Total reward: 95.0 Training loss: 5.3058 Explore P: 0.5437\n",
      "Episode: 165 Total reward: 95.0 Training loss: 9.3532 Explore P: 0.5434\n",
      "Model Saved\n",
      "Episode: 166 Total reward: 95.0 Training loss: 5.3769 Explore P: 0.5430\n",
      "Episode: 167 Total reward: 95.0 Training loss: 6.3074 Explore P: 0.5427\n",
      "Episode: 168 Total reward: -21.0 Training loss: 14.2721 Explore P: 0.5376\n",
      "Episode: 169 Total reward: 76.0 Training loss: 8.9669 Explore P: 0.5365\n",
      "Episode: 170 Total reward: 95.0 Training loss: 6.5394 Explore P: 0.5362\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 94.0 Training loss: 8.3638 Explore P: 0.5358\n",
      "Episode: 172 Total reward: 95.0 Training loss: 5.5448 Explore P: 0.5355\n",
      "Episode: 173 Total reward: 94.0 Training loss: 3.6157 Explore P: 0.5352\n",
      "Episode: 174 Total reward: 94.0 Training loss: 14.1364 Explore P: 0.5348\n",
      "Episode: 175 Total reward: 63.0 Training loss: 17.9014 Explore P: 0.5331\n",
      "Model Saved\n",
      "Episode: 176 Total reward: 68.0 Training loss: 11.4577 Explore P: 0.5316\n",
      "Episode: 177 Total reward: 60.0 Training loss: 15.0636 Explore P: 0.5297\n",
      "Episode: 178 Total reward: 76.0 Training loss: 5.1174 Explore P: 0.5287\n",
      "Episode: 179 Total reward: 95.0 Training loss: 4.2875 Explore P: 0.5284\n",
      "Model Saved\n",
      "Episode: 181 Total reward: 72.0 Training loss: 3.0206 Explore P: 0.5220\n",
      "Episode: 182 Total reward: 95.0 Training loss: 9.2020 Explore P: 0.5217\n",
      "Episode: 183 Total reward: 90.0 Training loss: 7.4530 Explore P: 0.5211\n",
      "Episode: 184 Total reward: 72.0 Training loss: 8.0814 Explore P: 0.5199\n",
      "Episode: 185 Total reward: 93.0 Training loss: 2.6691 Explore P: 0.5195\n",
      "Model Saved\n",
      "Episode: 186 Total reward: 93.0 Training loss: 9.9340 Explore P: 0.5191\n",
      "Episode: 187 Total reward: 95.0 Training loss: 7.1898 Explore P: 0.5188\n",
      "Episode: 188 Total reward: 95.0 Training loss: 5.4221 Explore P: 0.5185\n",
      "Episode: 189 Total reward: 93.0 Training loss: 7.6535 Explore P: 0.5181\n",
      "Episode: 190 Total reward: 95.0 Training loss: 7.2413 Explore P: 0.5178\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 95.0 Training loss: 11.1568 Explore P: 0.5175\n",
      "Episode: 192 Total reward: 41.0 Training loss: 7.0685 Explore P: 0.5149\n",
      "Episode: 193 Total reward: 95.0 Training loss: 6.4974 Explore P: 0.5146\n",
      "Episode: 195 Total reward: 30.0 Training loss: 7.7666 Explore P: 0.5068\n",
      "Model Saved\n",
      "Episode: 196 Total reward: 95.0 Training loss: 10.0580 Explore P: 0.5065\n",
      "Episode: 197 Total reward: 89.0 Training loss: 10.1030 Explore P: 0.5059\n",
      "Episode: 198 Total reward: 36.0 Training loss: 12.0402 Explore P: 0.5032\n",
      "Episode: 199 Total reward: 29.0 Training loss: 7.7697 Explore P: 0.5004\n",
      "Episode: 200 Total reward: 95.0 Training loss: 12.0135 Explore P: 0.5001\n",
      "Model Saved\n",
      "Episode: 201 Total reward: 90.0 Training loss: 7.0591 Explore P: 0.4996\n",
      "Episode: 202 Total reward: 95.0 Training loss: 45.5382 Explore P: 0.4993\n",
      "Episode: 203 Total reward: 95.0 Training loss: 5.2836 Explore P: 0.4990\n",
      "Episode: 204 Total reward: 95.0 Training loss: 3.9595 Explore P: 0.4987\n",
      "Episode: 205 Total reward: 94.0 Training loss: 12.6953 Explore P: 0.4983\n",
      "Model Saved\n",
      "Episode: 206 Total reward: 95.0 Training loss: 6.2718 Explore P: 0.4980\n",
      "Episode: 207 Total reward: 32.0 Training loss: 2.5679 Explore P: 0.4954\n",
      "Episode: 208 Total reward: 95.0 Training loss: 15.2178 Explore P: 0.4951\n",
      "Episode: 209 Total reward: 56.0 Training loss: 4.8639 Explore P: 0.4934\n",
      "Episode: 210 Total reward: 95.0 Training loss: 8.3517 Explore P: 0.4931\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 23.0 Training loss: 7.4240 Explore P: 0.4901\n",
      "Episode: 212 Total reward: 88.0 Training loss: 4.7976 Explore P: 0.4895\n",
      "Episode: 213 Total reward: 30.0 Training loss: 6.1415 Explore P: 0.4866\n",
      "Episode: 214 Total reward: 92.0 Training loss: 15.4500 Explore P: 0.4861\n",
      "Episode: 215 Total reward: 95.0 Training loss: 9.9634 Explore P: 0.4859\n",
      "Model Saved\n",
      "Episode: 216 Total reward: 95.0 Training loss: 10.8063 Explore P: 0.4856\n",
      "Episode: 217 Total reward: 53.0 Training loss: 8.1193 Explore P: 0.4838\n",
      "Episode: 218 Total reward: 18.0 Training loss: 9.4159 Explore P: 0.4806\n",
      "Episode: 219 Total reward: 48.0 Training loss: 8.6391 Explore P: 0.4785\n",
      "Episode: 220 Total reward: 95.0 Training loss: 6.1838 Explore P: 0.4783\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 95.0 Training loss: 6.9819 Explore P: 0.4780\n",
      "Episode: 222 Total reward: 46.0 Training loss: 7.3177 Explore P: 0.4759\n",
      "Episode: 223 Total reward: 92.0 Training loss: 10.8050 Explore P: 0.4755\n",
      "Episode: 224 Total reward: 95.0 Training loss: 6.7800 Explore P: 0.4752\n",
      "Model Saved\n",
      "Episode: 226 Total reward: 94.0 Training loss: 10.2422 Explore P: 0.4702\n",
      "Episode: 227 Total reward: 50.0 Training loss: 8.4003 Explore P: 0.4683\n",
      "Episode: 228 Total reward: 73.0 Training loss: 5.0683 Explore P: 0.4673\n",
      "Episode: 229 Total reward: 69.0 Training loss: 8.8295 Explore P: 0.4661\n",
      "Episode: 230 Total reward: 64.0 Training loss: 6.1810 Explore P: 0.4646\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 94.0 Training loss: 8.5951 Explore P: 0.4643\n",
      "Episode: 232 Total reward: 93.0 Training loss: 8.2481 Explore P: 0.4639\n",
      "Episode: 233 Total reward: 94.0 Training loss: 9.3168 Explore P: 0.4636\n",
      "Episode: 234 Total reward: 94.0 Training loss: 5.7313 Explore P: 0.4633\n",
      "Episode: 235 Total reward: 95.0 Training loss: 8.3524 Explore P: 0.4630\n",
      "Model Saved\n",
      "Episode: 236 Total reward: 95.0 Training loss: 3.2534 Explore P: 0.4627\n",
      "Episode: 237 Total reward: 67.0 Training loss: 4.9519 Explore P: 0.4614\n",
      "Episode: 238 Total reward: 95.0 Training loss: 4.9785 Explore P: 0.4612\n",
      "Episode: 239 Total reward: 94.0 Training loss: 6.1041 Explore P: 0.4608\n",
      "Episode: 240 Total reward: 89.0 Training loss: 6.4602 Explore P: 0.4603\n",
      "Model Saved\n",
      "Episode: 242 Total reward: 72.0 Training loss: 7.2753 Explore P: 0.4547\n",
      "Episode: 243 Total reward: 7.0 Training loss: 4.0682 Explore P: 0.4512\n",
      "Episode: 244 Total reward: 95.0 Training loss: 5.4801 Explore P: 0.4510\n",
      "Model Saved\n",
      "Episode: 247 Total reward: 92.0 Training loss: 4.0671 Explore P: 0.4419\n",
      "Episode: 248 Total reward: 94.0 Training loss: 10.1769 Explore P: 0.4416\n",
      "Episode: 249 Total reward: 95.0 Training loss: 7.8524 Explore P: 0.4413\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 94.0 Training loss: 3.7941 Explore P: 0.4367\n",
      "Episode: 252 Total reward: 95.0 Training loss: 5.4722 Explore P: 0.4365\n",
      "Episode: 253 Total reward: 23.0 Training loss: 6.2462 Explore P: 0.4338\n",
      "Episode: 254 Total reward: 94.0 Training loss: 8.9440 Explore P: 0.4335\n",
      "Episode: 255 Total reward: 95.0 Training loss: 7.0957 Explore P: 0.4332\n",
      "Model Saved\n",
      "Episode: 256 Total reward: 93.0 Training loss: 6.5590 Explore P: 0.4329\n",
      "Episode: 257 Total reward: 95.0 Training loss: 9.4697 Explore P: 0.4326\n",
      "Episode: 258 Total reward: 93.0 Training loss: 4.4644 Explore P: 0.4323\n",
      "Episode: 259 Total reward: 95.0 Training loss: 5.4563 Explore P: 0.4320\n",
      "Episode: 260 Total reward: 65.0 Training loss: 5.4484 Explore P: 0.4307\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 59.0 Training loss: 6.1749 Explore P: 0.4292\n",
      "Episode: 262 Total reward: 50.0 Training loss: 9.7505 Explore P: 0.4275\n",
      "Episode: 263 Total reward: 75.0 Training loss: 13.6371 Explore P: 0.4266\n",
      "Episode: 264 Total reward: 71.0 Training loss: 4.7094 Explore P: 0.4256\n",
      "Episode: 265 Total reward: 45.0 Training loss: 7.2052 Explore P: 0.4236\n",
      "Model Saved\n",
      "Episode: 266 Total reward: 70.0 Training loss: 7.2868 Explore P: 0.4226\n",
      "Episode: 267 Total reward: 95.0 Training loss: 23.1591 Explore P: 0.4223\n",
      "Episode: 268 Total reward: 92.0 Training loss: 5.6330 Explore P: 0.4220\n",
      "Episode: 269 Total reward: 95.0 Training loss: 8.5673 Explore P: 0.4217\n",
      "Episode: 270 Total reward: 91.0 Training loss: 8.4516 Explore P: 0.4213\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 46.0 Training loss: 37.7152 Explore P: 0.4194\n",
      "Episode: 272 Total reward: 94.0 Training loss: 8.5222 Explore P: 0.4192\n",
      "Episode: 273 Total reward: 51.0 Training loss: 6.2626 Explore P: 0.4173\n",
      "Episode: 274 Total reward: 29.0 Training loss: 6.5222 Explore P: 0.4150\n",
      "Episode: 275 Total reward: 95.0 Training loss: 7.2604 Explore P: 0.4148\n",
      "Model Saved\n",
      "Episode: 276 Total reward: 60.0 Training loss: 6.1872 Explore P: 0.4133\n",
      "Episode: 277 Total reward: 91.0 Training loss: 10.2827 Explore P: 0.4129\n",
      "Episode: 278 Total reward: 76.0 Training loss: 6.5719 Explore P: 0.4121\n",
      "Episode: 279 Total reward: 95.0 Training loss: 5.4216 Explore P: 0.4119\n",
      "Episode: 280 Total reward: 95.0 Training loss: 8.4445 Explore P: 0.4116\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 94.0 Training loss: 6.8454 Explore P: 0.4113\n",
      "Episode: 282 Total reward: 95.0 Training loss: 4.1795 Explore P: 0.4111\n",
      "Episode: 283 Total reward: 95.0 Training loss: 15.3680 Explore P: 0.4109\n",
      "Episode: 284 Total reward: 95.0 Training loss: 8.8869 Explore P: 0.4106\n",
      "Episode: 285 Total reward: 95.0 Training loss: 4.3235 Explore P: 0.4104\n",
      "Model Saved\n",
      "Episode: 286 Total reward: 31.0 Training loss: 9.5937 Explore P: 0.4080\n",
      "Episode: 287 Total reward: 69.0 Training loss: 9.4985 Explore P: 0.4069\n",
      "Episode: 288 Total reward: 76.0 Training loss: 9.0317 Explore P: 0.4061\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 95.0 Training loss: 5.4898 Explore P: 0.3980\n",
      "Episode: 292 Total reward: 57.0 Training loss: 4.8576 Explore P: 0.3967\n",
      "Episode: 293 Total reward: 95.0 Training loss: 4.5825 Explore P: 0.3965\n",
      "Episode: 294 Total reward: 94.0 Training loss: 8.1916 Explore P: 0.3962\n",
      "Model Saved\n",
      "Episode: 296 Total reward: 32.0 Training loss: 4.2516 Explore P: 0.3903\n",
      "Episode: 297 Total reward: 95.0 Training loss: 9.9330 Explore P: 0.3901\n",
      "Episode: 298 Total reward: 95.0 Training loss: 8.1190 Explore P: 0.3899\n",
      "Episode: 300 Total reward: 92.0 Training loss: 36.0580 Explore P: 0.3857\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 94.0 Training loss: 25.6411 Explore P: 0.3855\n",
      "Episode: 302 Total reward: 95.0 Training loss: 7.0234 Explore P: 0.3853\n",
      "Episode: 303 Total reward: 95.0 Training loss: 6.9925 Explore P: 0.3850\n",
      "Episode: 304 Total reward: 48.0 Training loss: 3.7165 Explore P: 0.3834\n",
      "Episode: 305 Total reward: 94.0 Training loss: 5.5598 Explore P: 0.3832\n",
      "Model Saved\n",
      "Episode: 306 Total reward: 66.0 Training loss: 6.4380 Explore P: 0.3820\n",
      "Episode: 307 Total reward: 95.0 Training loss: 10.1027 Explore P: 0.3818\n",
      "Episode: 308 Total reward: 76.0 Training loss: 4.8863 Explore P: 0.3811\n",
      "Episode: 309 Total reward: 91.0 Training loss: 3.8596 Explore P: 0.3807\n",
      "Episode: 310 Total reward: 95.0 Training loss: 5.7696 Explore P: 0.3805\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 95.0 Training loss: 7.9453 Explore P: 0.3803\n",
      "Episode: 312 Total reward: 94.0 Training loss: 32.3577 Explore P: 0.3800\n",
      "Episode: 313 Total reward: 94.0 Training loss: 6.2316 Explore P: 0.3797\n",
      "Episode: 314 Total reward: 95.0 Training loss: 5.6629 Explore P: 0.3795\n",
      "Episode: 315 Total reward: 51.0 Training loss: 16.0406 Explore P: 0.3780\n",
      "Model Saved\n",
      "Episode: 316 Total reward: 64.0 Training loss: 20.9551 Explore P: 0.3769\n",
      "Episode: 317 Total reward: 95.0 Training loss: 8.9643 Explore P: 0.3766\n",
      "Episode: 318 Total reward: 31.0 Training loss: 10.1815 Explore P: 0.3745\n",
      "Episode: 319 Total reward: 95.0 Training loss: 6.8077 Explore P: 0.3742\n",
      "Episode: 320 Total reward: 69.0 Training loss: 6.2630 Explore P: 0.3733\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 64.0 Training loss: 9.0876 Explore P: 0.3721\n",
      "Episode: 322 Total reward: 65.0 Training loss: 6.7376 Explore P: 0.3710\n",
      "Episode: 325 Total reward: 95.0 Training loss: 6.3275 Explore P: 0.3636\n",
      "Model Saved\n",
      "Episode: 326 Total reward: 61.0 Training loss: 21.0627 Explore P: 0.3624\n",
      "Episode: 327 Total reward: 95.0 Training loss: 8.4630 Explore P: 0.3622\n",
      "Episode: 328 Total reward: 94.0 Training loss: 4.1237 Explore P: 0.3619\n",
      "Episode: 329 Total reward: 95.0 Training loss: 7.0530 Explore P: 0.3617\n",
      "Episode: 330 Total reward: 95.0 Training loss: 5.6433 Explore P: 0.3615\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 68.0 Training loss: 7.4114 Explore P: 0.3605\n",
      "Episode: 332 Total reward: 94.0 Training loss: 4.9424 Explore P: 0.3603\n",
      "Episode: 333 Total reward: 95.0 Training loss: 6.4706 Explore P: 0.3601\n",
      "Episode: 334 Total reward: 95.0 Training loss: 10.3608 Explore P: 0.3599\n",
      "Episode: 335 Total reward: 95.0 Training loss: 3.5082 Explore P: 0.3596\n",
      "Model Saved\n",
      "Episode: 336 Total reward: 94.0 Training loss: 7.4861 Explore P: 0.3594\n",
      "Episode: 337 Total reward: 95.0 Training loss: 6.5843 Explore P: 0.3592\n",
      "Episode: 339 Total reward: 95.0 Training loss: 6.2024 Explore P: 0.3555\n",
      "Episode: 340 Total reward: 56.0 Training loss: 17.9740 Explore P: 0.3543\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 93.0 Training loss: 6.3163 Explore P: 0.3540\n",
      "Episode: 342 Total reward: 95.0 Training loss: 4.4607 Explore P: 0.3538\n",
      "Episode: 343 Total reward: 46.0 Training loss: 12.3406 Explore P: 0.3523\n",
      "Episode: 344 Total reward: 95.0 Training loss: 4.6383 Explore P: 0.3521\n",
      "Episode: 345 Total reward: 49.0 Training loss: 5.2035 Explore P: 0.3506\n",
      "Model Saved\n",
      "Episode: 346 Total reward: 62.0 Training loss: 4.3100 Explore P: 0.3495\n",
      "Episode: 347 Total reward: 35.0 Training loss: 7.4819 Explore P: 0.3476\n",
      "Episode: 349 Total reward: 32.0 Training loss: 6.7938 Explore P: 0.3424\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 70.0 Training loss: 13.7135 Explore P: 0.3383\n",
      "Episode: 352 Total reward: 95.0 Training loss: 5.8986 Explore P: 0.3381\n",
      "Episode: 353 Total reward: 95.0 Training loss: 6.7222 Explore P: 0.3379\n",
      "Episode: 354 Total reward: 95.0 Training loss: 5.5502 Explore P: 0.3377\n",
      "Episode: 355 Total reward: 95.0 Training loss: 3.9625 Explore P: 0.3375\n",
      "Model Saved\n",
      "Episode: 356 Total reward: 66.0 Training loss: 10.4589 Explore P: 0.3365\n",
      "Episode: 357 Total reward: 95.0 Training loss: 4.2301 Explore P: 0.3363\n",
      "Episode: 358 Total reward: 43.0 Training loss: 6.8768 Explore P: 0.3347\n",
      "Episode: 359 Total reward: 27.0 Training loss: 10.0385 Explore P: 0.3328\n",
      "Episode: 360 Total reward: 95.0 Training loss: 6.9034 Explore P: 0.3326\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 95.0 Training loss: 5.8895 Explore P: 0.3324\n",
      "Episode: 362 Total reward: 94.0 Training loss: 7.4681 Explore P: 0.3322\n",
      "Episode: 363 Total reward: 95.0 Training loss: 6.9938 Explore P: 0.3320\n",
      "Episode: 364 Total reward: 94.0 Training loss: 4.1534 Explore P: 0.3318\n",
      "Episode: 365 Total reward: 48.0 Training loss: 11.6158 Explore P: 0.3304\n",
      "Model Saved\n",
      "Episode: 366 Total reward: 94.0 Training loss: 3.2904 Explore P: 0.3302\n",
      "Episode: 368 Total reward: 93.0 Training loss: 5.1160 Explore P: 0.3267\n",
      "Episode: 369 Total reward: 95.0 Training loss: 12.5915 Explore P: 0.3266\n",
      "Episode: 370 Total reward: 95.0 Training loss: 4.7789 Explore P: 0.3264\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 94.0 Training loss: 4.6793 Explore P: 0.3261\n",
      "Episode: 372 Total reward: 23.0 Training loss: 5.5631 Explore P: 0.3242\n",
      "Episode: 373 Total reward: 54.0 Training loss: 6.1912 Explore P: 0.3228\n",
      "Episode: 374 Total reward: 47.0 Training loss: 5.8507 Explore P: 0.3215\n",
      "Episode: 375 Total reward: 95.0 Training loss: 38.6352 Explore P: 0.3213\n",
      "Model Saved\n",
      "Episode: 376 Total reward: 94.0 Training loss: 3.8564 Explore P: 0.3211\n",
      "Episode: 377 Total reward: 95.0 Training loss: 3.2192 Explore P: 0.3209\n",
      "Episode: 378 Total reward: 67.0 Training loss: 10.6641 Explore P: 0.3200\n",
      "Episode: 379 Total reward: 74.0 Training loss: 6.4169 Explore P: 0.3193\n",
      "Episode: 380 Total reward: 24.0 Training loss: 5.3259 Explore P: 0.3174\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 94.0 Training loss: 8.3347 Explore P: 0.3172\n",
      "Episode: 383 Total reward: 95.0 Training loss: 5.7760 Explore P: 0.3139\n",
      "Episode: 384 Total reward: 95.0 Training loss: 6.2676 Explore P: 0.3137\n",
      "Episode: 385 Total reward: 94.0 Training loss: 8.2629 Explore P: 0.3135\n",
      "Model Saved\n",
      "Episode: 386 Total reward: 93.0 Training loss: 13.1794 Explore P: 0.3133\n",
      "Episode: 387 Total reward: 95.0 Training loss: 16.9010 Explore P: 0.3131\n",
      "Episode: 388 Total reward: 71.0 Training loss: 5.7769 Explore P: 0.3124\n",
      "Episode: 389 Total reward: 95.0 Training loss: 8.5311 Explore P: 0.3122\n",
      "Episode: 390 Total reward: 95.0 Training loss: 11.1890 Explore P: 0.3120\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 95.0 Training loss: 8.7124 Explore P: 0.3118\n",
      "Episode: 392 Total reward: 41.0 Training loss: 4.5517 Explore P: 0.3103\n",
      "Episode: 393 Total reward: 73.0 Training loss: 11.3653 Explore P: 0.3096\n",
      "Episode: 394 Total reward: 94.0 Training loss: 4.2256 Explore P: 0.3094\n",
      "Episode: 395 Total reward: 76.0 Training loss: 11.7555 Explore P: 0.3088\n",
      "Model Saved\n",
      "Episode: 396 Total reward: 95.0 Training loss: 9.5749 Explore P: 0.3086\n",
      "Episode: 397 Total reward: 95.0 Training loss: 4.6111 Explore P: 0.3085\n",
      "Episode: 398 Total reward: 46.0 Training loss: 5.3761 Explore P: 0.3071\n",
      "Episode: 399 Total reward: 95.0 Training loss: 6.5730 Explore P: 0.3069\n",
      "Episode: 400 Total reward: 53.0 Training loss: 2.7583 Explore P: 0.3058\n",
      "Model Saved\n",
      "Episode: 403 Total reward: 26.0 Training loss: 3.1875 Explore P: 0.2982\n",
      "Episode: 404 Total reward: 95.0 Training loss: 4.3228 Explore P: 0.2980\n",
      "Episode: 405 Total reward: 94.0 Training loss: 5.4822 Explore P: 0.2978\n",
      "Model Saved\n",
      "Episode: 406 Total reward: 69.0 Training loss: 8.0316 Explore P: 0.2971\n",
      "Episode: 407 Total reward: 95.0 Training loss: 5.3825 Explore P: 0.2969\n",
      "Episode: 408 Total reward: 95.0 Training loss: 4.2439 Explore P: 0.2967\n",
      "Episode: 409 Total reward: 95.0 Training loss: 5.8542 Explore P: 0.2965\n",
      "Episode: 410 Total reward: 95.0 Training loss: 7.2658 Explore P: 0.2964\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 92.0 Training loss: 3.3414 Explore P: 0.2961\n",
      "Episode: 412 Total reward: 95.0 Training loss: 8.9129 Explore P: 0.2959\n",
      "Episode: 413 Total reward: 95.0 Training loss: 4.9458 Explore P: 0.2958\n",
      "Episode: 414 Total reward: 95.0 Training loss: 10.0100 Explore P: 0.2956\n",
      "Episode: 415 Total reward: 76.0 Training loss: 6.3239 Explore P: 0.2950\n",
      "Model Saved\n",
      "Episode: 416 Total reward: 71.0 Training loss: 4.9018 Explore P: 0.2943\n",
      "Episode: 417 Total reward: 95.0 Training loss: 4.4679 Explore P: 0.2942\n",
      "Episode: 418 Total reward: 65.0 Training loss: 6.4482 Explore P: 0.2933\n",
      "Episode: 420 Total reward: 95.0 Training loss: 7.1073 Explore P: 0.2903\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 93.0 Training loss: 6.0036 Explore P: 0.2901\n",
      "Episode: 422 Total reward: 95.0 Training loss: 9.8681 Explore P: 0.2899\n",
      "Episode: 423 Total reward: 56.0 Training loss: 7.0279 Explore P: 0.2888\n",
      "Episode: 424 Total reward: 69.0 Training loss: 4.3213 Explore P: 0.2880\n",
      "Episode: 425 Total reward: 95.0 Training loss: 3.7574 Explore P: 0.2879\n",
      "Model Saved\n",
      "Episode: 426 Total reward: 95.0 Training loss: 6.3303 Explore P: 0.2877\n",
      "Episode: 427 Total reward: 65.0 Training loss: 6.7198 Explore P: 0.2868\n",
      "Episode: 428 Total reward: 95.0 Training loss: 6.6766 Explore P: 0.2867\n",
      "Episode: 429 Total reward: 95.0 Training loss: 17.5487 Explore P: 0.2865\n",
      "Episode: 430 Total reward: 92.0 Training loss: 4.6535 Explore P: 0.2862\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 94.0 Training loss: 6.1203 Explore P: 0.2861\n",
      "Episode: 432 Total reward: 48.0 Training loss: 4.3775 Explore P: 0.2849\n",
      "Episode: 433 Total reward: 95.0 Training loss: 3.8334 Explore P: 0.2847\n",
      "Episode: 435 Total reward: 95.0 Training loss: 4.2451 Explore P: 0.2818\n",
      "Model Saved\n",
      "Episode: 436 Total reward: 94.0 Training loss: 6.8777 Explore P: 0.2816\n",
      "Episode: 437 Total reward: 75.0 Training loss: 6.9144 Explore P: 0.2811\n",
      "Episode: 438 Total reward: 95.0 Training loss: 6.6239 Explore P: 0.2809\n",
      "Episode: 439 Total reward: 33.0 Training loss: 8.7485 Explore P: 0.2795\n",
      "Episode: 440 Total reward: 50.0 Training loss: 3.3633 Explore P: 0.2784\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 76.0 Training loss: 4.0757 Explore P: 0.2778\n",
      "Episode: 442 Total reward: 95.0 Training loss: 16.1746 Explore P: 0.2777\n",
      "Episode: 443 Total reward: 95.0 Training loss: 6.0739 Explore P: 0.2775\n",
      "Episode: 444 Total reward: 93.0 Training loss: 5.0369 Explore P: 0.2773\n",
      "Episode: 445 Total reward: 95.0 Training loss: 4.8247 Explore P: 0.2771\n",
      "Model Saved\n",
      "Episode: 446 Total reward: 95.0 Training loss: 5.8592 Explore P: 0.2770\n",
      "Episode: 447 Total reward: 95.0 Training loss: 9.7572 Explore P: 0.2768\n",
      "Episode: 448 Total reward: 95.0 Training loss: 14.4917 Explore P: 0.2766\n",
      "Episode: 449 Total reward: 95.0 Training loss: 5.8893 Explore P: 0.2765\n",
      "Episode: 450 Total reward: 92.0 Training loss: 6.8390 Explore P: 0.2762\n",
      "Model Saved\n",
      "Episode: 452 Total reward: 25.0 Training loss: 7.2424 Explore P: 0.2719\n",
      "Episode: 453 Total reward: 95.0 Training loss: 3.3932 Explore P: 0.2717\n",
      "Episode: 454 Total reward: 33.0 Training loss: 4.5696 Explore P: 0.2702\n",
      "Episode: 455 Total reward: 94.0 Training loss: 4.2600 Explore P: 0.2700\n",
      "Model Saved\n",
      "Episode: 456 Total reward: 95.0 Training loss: 3.8030 Explore P: 0.2699\n",
      "Episode: 457 Total reward: 95.0 Training loss: 6.3787 Explore P: 0.2697\n",
      "Episode: 458 Total reward: 64.0 Training loss: 9.2199 Explore P: 0.2689\n",
      "Episode: 459 Total reward: 94.0 Training loss: 4.4109 Explore P: 0.2687\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 93.0 Training loss: 3.8496 Explore P: 0.2659\n",
      "Episode: 462 Total reward: 95.0 Training loss: 9.1421 Explore P: 0.2658\n",
      "Episode: 463 Total reward: 87.0 Training loss: 8.9370 Explore P: 0.2654\n",
      "Episode: 464 Total reward: 95.0 Training loss: 4.4315 Explore P: 0.2652\n",
      "Episode: 465 Total reward: 46.0 Training loss: 5.9007 Explore P: 0.2641\n",
      "Model Saved\n",
      "Episode: 466 Total reward: 95.0 Training loss: 9.2814 Explore P: 0.2639\n",
      "Episode: 467 Total reward: 95.0 Training loss: 8.5237 Explore P: 0.2638\n",
      "Episode: 468 Total reward: 95.0 Training loss: 3.9001 Explore P: 0.2636\n",
      "Episode: 469 Total reward: 74.0 Training loss: 5.5425 Explore P: 0.2631\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 95.0 Training loss: 14.9009 Explore P: 0.2604\n",
      "Episode: 472 Total reward: 71.0 Training loss: 17.0582 Explore P: 0.2598\n",
      "Episode: 473 Total reward: 54.0 Training loss: 5.3778 Explore P: 0.2589\n",
      "Episode: 475 Total reward: 95.0 Training loss: 4.9826 Explore P: 0.2562\n",
      "Model Saved\n",
      "Episode: 476 Total reward: 95.0 Training loss: 5.3319 Explore P: 0.2561\n",
      "Episode: 478 Total reward: 42.0 Training loss: 3.6568 Explore P: 0.2525\n",
      "Episode: 479 Total reward: 67.0 Training loss: 4.7285 Explore P: 0.2518\n",
      "Episode: 480 Total reward: 95.0 Training loss: 12.3142 Explore P: 0.2516\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 63.0 Training loss: 4.9407 Explore P: 0.2508\n",
      "Episode: 482 Total reward: 95.0 Training loss: 7.1913 Explore P: 0.2507\n",
      "Episode: 483 Total reward: 63.0 Training loss: 8.3169 Explore P: 0.2499\n",
      "Episode: 484 Total reward: 94.0 Training loss: 3.5713 Explore P: 0.2497\n",
      "Episode: 485 Total reward: 53.0 Training loss: 6.2106 Explore P: 0.2488\n",
      "Model Saved\n",
      "Episode: 486 Total reward: 95.0 Training loss: 5.9556 Explore P: 0.2487\n",
      "Episode: 488 Total reward: 71.0 Training loss: 6.7643 Explore P: 0.2457\n",
      "Episode: 489 Total reward: 65.0 Training loss: 13.9713 Explore P: 0.2450\n",
      "Episode: 490 Total reward: 74.0 Training loss: 7.3133 Explore P: 0.2444\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 95.0 Training loss: 5.5533 Explore P: 0.2443\n",
      "Episode: 492 Total reward: 75.0 Training loss: 4.2623 Explore P: 0.2438\n",
      "Episode: 493 Total reward: 94.0 Training loss: 36.9924 Explore P: 0.2436\n",
      "Episode: 494 Total reward: 95.0 Training loss: 6.5324 Explore P: 0.2435\n",
      "Episode: 495 Total reward: 94.0 Training loss: 4.5361 Explore P: 0.2433\n",
      "Model Saved\n",
      "Episode: 496 Total reward: 52.0 Training loss: 5.9572 Explore P: 0.2424\n",
      "Episode: 497 Total reward: 72.0 Training loss: 4.9778 Explore P: 0.2419\n",
      "Episode: 498 Total reward: 95.0 Training loss: 33.0955 Explore P: 0.2417\n",
      "Episode: 499 Total reward: 95.0 Training loss: 6.1322 Explore P: 0.2416\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 9: Watch our Agent play üëÄ\n",
    "Now that we trained our agent, we can test it"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "with tf.Session() as sess:\r\n",
    "    \r\n",
    "    game, possible_actions = create_environment()\r\n",
    "    \r\n",
    "    totalScore = 0\r\n",
    "    \r\n",
    "    # Load the model\r\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\r\n",
    "    game.init()\r\n",
    "    for i in range(1):\r\n",
    "        \r\n",
    "        done = False\r\n",
    "        \r\n",
    "        game.new_episode()\r\n",
    "        \r\n",
    "        state = game.get_state().screen_buffer\r\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\r\n",
    "            \r\n",
    "        while not game.is_episode_finished():\r\n",
    "            # Take the biggest Q value (= the best action)\r\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\r\n",
    "            \r\n",
    "            # Take the biggest Q value (= the best action)\r\n",
    "            choice = np.argmax(Qs)\r\n",
    "            action = possible_actions[int(choice)]\r\n",
    "            \r\n",
    "            game.make_action(action)\r\n",
    "            done = game.is_episode_finished()\r\n",
    "            score = game.get_total_reward()\r\n",
    "            \r\n",
    "            if done:\r\n",
    "                break  \r\n",
    "                \r\n",
    "            else:\r\n",
    "                print(\"else\")\r\n",
    "                next_state = game.get_state().screen_buffer\r\n",
    "                next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\r\n",
    "                state = next_state\r\n",
    "                \r\n",
    "        score = game.get_total_reward()\r\n",
    "        print(\"Score: \", score)\r\n",
    "    game.close()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "else\n",
      "Score:  68.0\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('python_3.7.4': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "92541e99ce5daeb319a09956659047126cf50de395682e6556b4e42a57122077"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}