{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ecf5722fdaf1897a315d257d89d94520bfcaa453217d5becf09b39e73618b0de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import os\n",
    "  \n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "\n",
    "# convert mp3 file to wav  \n",
    "# src=(\"C:\\\\Users\\\\pyjpa\\\\Desktop\\\\22.mp3\")\n",
    "# sound = AudioSegment.from_mp3(src)\n",
    "# sound.export(\"C:\\\\Users\\\\pyjpa\\\\Desktop\\\\22.flac\", format=\"flac\")\n",
    "\n",
    "file_audio = sr.AudioFile(\"C:\\\\Users\\\\pyjpa\\\\Desktop\\\\22.flac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the audio file as the audio source                                        \n",
    "r = sr.Recognizer()\n",
    "with file_audio as source:\n",
    "    audio_text = r.record(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<speech_recognition.AudioData at 0x1303154ea90>"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "audio_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = r.recognize_sphinx(audio_text, language=\"zh-CN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'将 自己 的 脚'"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silence_based_conversion(path=\"alice-medium.wav\"):\n",
    "    song = AudioSegment.from_wav(path)\n",
    "\n",
    "    fh = open(\"recognized.txt\", \"w\")\n",
    "\n",
    "    chunks = split_on_silence(song,\n",
    "        # must be silent for at least 0.5 seconds\n",
    "        # or 500 ms. adjust this value based on user\n",
    "        # requirement. if the speaker stays silent for \n",
    "        # longer, increase this value. else, decrease it.\n",
    "        min_silence_len = 500,\n",
    "  \n",
    "        # consider it silent if quieter than -16 dBFS\n",
    "        # adjust this per requirement\n",
    "        silence_thresh = -25\n",
    "    )\n",
    "  \n",
    "    # create a directory to store the audio chunks.\n",
    "    try:\n",
    "        os.mkdir('audio_chunks')\n",
    "    except(FileExistsError):\n",
    "        pass\n",
    "  \n",
    "    os.chdir('audio_chunks')\n",
    "    \n",
    "    print(chunks)\n",
    "\n",
    "    i = 0\n",
    "    # process each chunk\n",
    "    for chunk in chunks:\n",
    "              \n",
    "        # Create 0.5 seconds silence chunk\n",
    "        chunk_silent = AudioSegment.silent(duration = 10)\n",
    "  \n",
    "        # add 0.5 sec silence to beginning and \n",
    "        # end of audio chunk. This is done so that\n",
    "        # it doesn't seem abruptly sliced.\n",
    "        audio_chunk = chunk_silent + chunk + chunk_silent\n",
    "  \n",
    "        # export audio chunk and save it in \n",
    "        # the current directory.\n",
    "        print(\"saving chunk{0}.wav\".format(i))\n",
    "        # specify the bitrate to be 192 k\n",
    "        audio_chunk.export(\"./chunk{0}.wav\".format(i), bitrate ='192k', format =\"wav\")\n",
    "  \n",
    "        # the name of the newly created chunk\n",
    "        filename = 'chunk'+str(i)+'.wav'\n",
    "  \n",
    "        print(\"Processing chunk \"+str(i))\n",
    "  \n",
    "        # get the name of the newly created chunk\n",
    "        # in the AUDIO_FILE variable for later use.\n",
    "        file = filename\n",
    "  \n",
    "        # create a speech recognition object\n",
    "        r = sr.Recognizer()\n",
    "  \n",
    "        # recognize the chunk\n",
    "        with sr.AudioFile(file) as source:\n",
    "            # remove this if it is not working\n",
    "            # correctly.\n",
    "            r.adjust_for_ambient_noise(source)\n",
    "            audio_listened = r.listen(source)\n",
    "  \n",
    "        try:\n",
    "            # try converting it to text\n",
    "            rec = r.recognize_sphinx(audio_listened)\n",
    "            # write the output to the file.\n",
    "            fh.write(rec+\". \")\n",
    "  \n",
    "        # catch any errors.\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "  \n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results. check your internet connection\")\n",
    "  \n",
    "        i += 1\n",
    "  \n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-e07a2abb9997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:\\\\Users\\\\pyjpa\\\\Desktop\\\\22.wav\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msilence_based_conversion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-fee73bfc06cc>\u001b[0m in \u001b[0;36msilence_based_conversion\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"recognized.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     chunks = split_on_silence(song,\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;31m# must be silent for at least 0.5 seconds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m# or 500 ms. adjust this value based on user\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pydub\\silence.py\u001b[0m in \u001b[0;36msplit_on_silence\u001b[1;34m(audio_segment, min_silence_len, silence_thresh, keep_silence, seek_step)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;33m[\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mkeep_silence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mkeep_silence\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m             \u001b[1;32min\u001b[0m \u001b[0mdetect_nonsilent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_segment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_silence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilence_thresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseek_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     ]\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pydub\\silence.py\u001b[0m in \u001b[0;36mdetect_nonsilent\u001b[1;34m(audio_segment, min_silence_len, silence_thresh, seek_step)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mseek_step\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstep\u001b[0m \u001b[0msize\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0minterating\u001b[0m \u001b[0mover\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msegment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \"\"\"\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0msilent_ranges\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_silence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_segment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_silence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilence_thresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseek_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[0mlen_seg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maudio_segment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\pydub\\silence.py\u001b[0m in \u001b[0;36mdetect_silence\u001b[1;34m(audio_segment, min_silence_len, silence_thresh, seek_step)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslice_starts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0maudio_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudio_segment\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmin_silence_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0maudio_slice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrms\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0msilence_thresh\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0msilence_starts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = \"C:\\\\Users\\\\pyjpa\\\\Desktop\\\\examples_chinese.flac\"\n",
    "silence_based_conversion(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}