{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"natural_language_processing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMx/KsxUDrn2M5QbIb03B9p"},"kernelspec":{"name":"python388jvsc74a57bd0e4a0114c34cb418e7d5a3697731935c08a51df47520424022bf18e448f4ec53a","display_name":"Python 3.8.8 64-bit ('base': conda)"}},"cells":[{"cell_type":"markdown","metadata":{"id":"VwK5-9FIB-lu","colab_type":"text"},"source":["# Natural Language Processing"]},{"cell_type":"markdown","metadata":{"id":"X1kiO9kACE6s","colab_type":"text"},"source":["## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"7QG7sxmoCIvN","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd"],"execution_count":123,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTfaCIzdCLPA","colab_type":"text"},"source":["## Importing the dataset"]},{"cell_type":"code","metadata":{"id":"UCK6vQ5QCQJe","colab_type":"code","colab":{}},"source":["dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)  # get rid of quotes"],"execution_count":124,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qekztq71CixT","colab_type":"text"},"source":["## Cleaning the texts"]},{"cell_type":"code","metadata":{"id":"8u_yXh9dCmEE","colab_type":"code","outputId":"bdcb9868-74c8-40b2-e5e9-877b949ce385","executionInfo":{"status":"ok","timestamp":1589837794372,"user_tz":-240,"elapsed":1977,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["import re\n","import nltk\n","# nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","corpus = []\n","for i in range(0, len(dataset)):\n","  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i]) # get rid of non letter to a space\n","  review = review.lower()\n","  review = review.split() # split review to a list of lower cases\n","  ps = PorterStemmer()  # take the root of the word\n","  all_stopwords = stopwords.words('english')\n","  all_stopwords.remove('not')\n","  review = [ps.stem(word) for word in review if word not in set(all_stopwords)]\n","  review = ' '.join(review)\n","  corpus.append(review)"],"execution_count":125,"outputs":[]},{"cell_type":"code","metadata":{"id":"KpGWdrzGoAsL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"a1d5020d-8005-4735-d4b9-ad99fb366534","executionInfo":{"status":"ok","timestamp":1589837797968,"user_tz":-240,"elapsed":942,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"tags":[]},"source":["# print(corpus)\n","print(set(all_stopwords))"],"execution_count":126,"outputs":[{"output_type":"stream","name":"stdout","text":["{'t', 'didn', 'too', \"you'd\", 'did', 'but', 'won', 'isn', 'they', 'before', 'if', 'above', \"don't\", 'each', 'has', 'me', 'weren', \"mightn't\", 'into', 'y', 'can', 'being', 'or', 'same', 'don', 'shouldn', 'a', 'below', \"you're\", 'll', 'd', 'as', 'o', 'he', 'at', 'that', 'until', 'had', 'when', 'both', 'such', 'here', 'once', 'itself', 'have', 'after', 'm', \"you'll\", 'will', 'where', 'under', 'do', 'was', 'in', 'my', \"needn't\", 'theirs', 'to', 'an', 'wasn', 'against', 'aren', 'there', 'this', 'out', 're', 'whom', 'down', 'couldn', 'for', 'again', \"shouldn't\", 'should', \"doesn't\", 'herself', 'by', 'on', 'does', 'be', 'hasn', 'all', 'hadn', 'shan', 'up', 'them', 'over', 'few', 'why', 'him', 'yourselves', 'ours', 'nor', 'further', \"haven't\", 'with', 'during', 'you', 'doing', 'mightn', 'these', 'some', 'most', 'been', \"wasn't\", 'myself', 'yourself', 'yours', 'those', 'ain', 'are', 'doesn', 'ourselves', 'who', 'we', \"it's\", 'needn', 'hers', 'more', 'she', 'having', 'were', 'no', \"hadn't\", 'between', 'ma', \"that'll\", 'how', 'while', \"shan't\", 'i', 'now', 'because', 'very', 'am', 'of', 'what', 'our', 'your', \"weren't\", 'which', 'from', 'its', 'their', 'so', \"you've\", 've', 'other', \"won't\", 'her', \"wouldn't\", \"should've\", 'wouldn', \"isn't\", \"couldn't\", 'through', 'than', 'then', 'and', 'his', 'the', 'is', 'mustn', 'it', 'any', 's', 'haven', 'off', \"hasn't\", 'just', 'himself', 'only', 'about', 'themselves', \"didn't\", 'own', \"mustn't\", \"aren't\", \"she's\"}\n"]}]},{"cell_type":"markdown","metadata":{"id":"CLqmAkANCp1-","colab_type":"text"},"source":["## Creating the Bag of Words model"]},{"cell_type":"code","metadata":{"id":"qroF7XcSCvY3","colab_type":"code","colab":{}},"source":["\"\"\"\n","Types:\n","Speech recognition (Audio frequency analysis)\n","Bag of words (classification)\n","CNN for text recognition\n","Seq2Seq\n","\n","Bag of words model:\n","bag array (each training data): [0, 0, 0, 0, 0, ..., 0] (20,000 elements)\n","every word (and some symbols) has a position in this list\n","last element includes words that can't be recognized (names, places, abbrivations...)\n","bag[word_position] + 1 if that word appear in the sentence\n","\n","using old email and responses(yes/no) as training data to predict a new message\n","(responses are simple)\n","\"\"\"\n","from sklearn.feature_extraction.text import CountVectorizer\n","cv = CountVectorizer(max_features = 2000)\n","X = cv.fit_transform(corpus).toarray()\n","y = dataset.iloc[:, -1].values"],"execution_count":127,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DH_VjgPzC2cd","colab_type":"text"},"source":["## Splitting the dataset into the Training set and Test set"]},{"cell_type":"code","metadata":{"id":"qQXYM5VzDDDI","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"],"execution_count":128,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VkIq23vEDIPt","colab_type":"text"},"source":["## Training the Naive Bayes model on the Training set"]},{"cell_type":"code","metadata":{"id":"DS9oiDXXDRdI","colab_type":"code","outputId":"77513c39-0ec6-4544-c056-26abe055b746","executionInfo":{"status":"ok","timestamp":1589791461906,"user_tz":-240,"elapsed":4465,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.naive_bayes import GaussianNB\n","classifier = GaussianNB()\n","classifier.fit(X_train, y_train)"],"execution_count":129,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GaussianNB()"]},"metadata":{},"execution_count":129}]},{"cell_type":"code","execution_count":130,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVC(random_state=0)"]},"metadata":{},"execution_count":130}],"source":["from sklearn.svm import SVC\n","classifier1 = SVC(kernel=\"rbf\", random_state=0)\n","classifier1.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":134,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"]},"metadata":{},"execution_count":134}],"source":["from sklearn.ensemble import RandomForestClassifier\n","classifier2 = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n","classifier2.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"1JaRM7zXDWUy","colab_type":"text"},"source":["## Predicting the Test set results"]},{"cell_type":"code","metadata":{"id":"Iif0CVhFDaMp","colab_type":"code","outputId":"1266c3f2-d500-440e-d756-e0eabad504a7","executionInfo":{"status":"ok","timestamp":1589791461907,"user_tz":-240,"elapsed":4464,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["y_pred = classifier.predict(X_test)\n","y_pred1 = classifier1.predict(X_test)\n","y_pred2 = classifier2.predict(X_test)\n","print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_pred1.reshape(len(y_pred), 1), y_pred2.reshape(len(y_test), 1), y_test.reshape(len(y_test), 1)),1))"],"execution_count":135,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 0 0 0]\n [1 0 0 0]\n [1 0 1 0]\n [0 0 0 0]\n [0 0 1 0]\n [1 0 0 0]\n [1 1 1 1]\n [1 0 0 0]\n [1 0 0 0]\n [1 1 0 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 0]\n [1 1 0 1]\n [1 1 1 1]\n [1 1 1 1]\n [0 0 1 0]\n [0 0 0 0]\n [0 0 0 0]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 1 1]\n [1 1 1 1]\n [1 0 0 0]\n [1 0 0 0]\n [0 1 1 1]\n [1 0 0 1]\n [1 1 1 1]\n [1 1 1 1]\n [0 0 0 0]\n [1 0 0 1]\n [1 0 0 1]\n [1 0 0 1]\n [1 0 0 1]\n [1 1 1 1]\n [0 0 0 0]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 0 0]\n [1 0 0 1]\n [0 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [1 1 1 0]\n [1 1 1 0]\n [0 0 1 0]\n [0 0 0 0]\n [1 1 1 1]\n [1 0 0 1]\n [1 1 0 1]\n [1 1 1 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 1]\n [1 0 0 1]\n [0 0 0 0]\n [1 1 0 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 0]\n [1 1 0 0]\n [1 0 1 1]\n [0 0 0 0]\n [1 1 0 1]\n [1 0 1 1]\n [1 0 0 1]\n [1 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 1 1]\n [0 0 0 0]\n [1 0 0 0]\n [1 0 0 1]\n [0 1 1 1]\n [0 0 0 0]\n [1 1 1 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 0 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 1 1 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 0 0]\n [1 0 0 0]\n [1 0 0 1]\n [1 1 0 0]\n [1 0 0 1]\n [1 0 0 1]\n [1 0 1 0]\n [0 0 0 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 0]\n [0 0 1 1]\n [1 0 0 0]\n [1 1 0 1]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 0 1]\n [0 0 0 1]\n [1 1 0 1]\n [0 0 0 0]\n [1 1 0 0]\n [1 0 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [1 0 0 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 1]\n [1 0 0 0]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 0 0]\n [0 1 1 1]\n [1 1 1 1]\n [1 1 0 1]\n [0 0 0 0]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 0]\n [1 0 0 1]\n [1 0 0 1]\n [1 0 1 1]\n [1 0 0 1]\n [0 0 0 1]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 0 0]\n [0 0 0 0]\n [1 1 1 1]\n [1 0 0 1]\n [1 1 1 1]\n [1 0 0 0]\n [1 0 0 0]\n [0 0 0 0]\n [0 1 1 1]\n [1 1 1 1]\n [0 1 1 0]\n [0 0 0 0]\n [1 0 0 0]\n [0 0 0 0]\n [0 0 0 0]\n [0 0 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 0 0]\n [1 0 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [0 0 0 0]\n [0 0 1 1]\n [1 1 1 1]\n [0 0 1 0]\n [0 0 0 0]\n [1 0 0 0]\n [0 0 0 0]\n [1 0 0 1]\n [0 0 0 0]\n [1 1 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 0 1]\n [0 0 0 0]\n [1 0 0 0]\n [1 0 0 0]\n [1 1 0 1]\n [0 0 0 0]\n [1 1 1 1]\n [1 1 1 1]\n [1 0 0 0]\n [1 0 0 1]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"xoMltea5Dir1","colab_type":"text"},"source":["## Making the Confusion Matrix"]},{"cell_type":"code","metadata":{"id":"Xj9IU6MxDnvo","colab_type":"code","outputId":"43efba29-9811-4913-a085-8355ec1c02cb","executionInfo":{"status":"ok","timestamp":1589791461907,"user_tz":-240,"elapsed":4462,"user":{"displayName":"Hadelin de Ponteves","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhEuXdT7eQweUmRPW8_laJuPggSK6hfvpl5a6WBaA=s64","userId":"15047218817161520419"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.metrics import confusion_matrix, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","print(accuracy_score(y_test, y_pred))\n","\n","\n","cm = confusion_matrix(y_test, y_pred1)\n","print(cm)\n","print(accuracy_score(y_test, y_pred1))\n","\n","\n","cm = confusion_matrix(y_test, y_pred2)\n","print(cm)\n","print(accuracy_score(y_test, y_pred2))"],"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["[[55 42]\n [12 91]]\n0.73\n[[89  8]\n [37 66]]\n0.775\n[[86 11]\n [43 60]]\n0.73\n"]}]}]}